{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the link\n",
    "link = \"https://www.youtube.com/watch?v=e049IoFBnLA\" \n",
    "# get the unique ID\n",
    "unique_id = link.split(\"=\")[-1]\n",
    "# get the transcript of the video based on the unique ID\n",
    "sub = YouTubeTranscriptApi.get_transcript(unique_id)  \n",
    "# get only the \"text\" parts and join them together, ignoring others like \"duration\"\n",
    "subtitle = \" \".join([x['text'] for x in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone. Hello everyone. Some of you dear contestants \n",
      "are very young so perhaps you do not know who   Professor Terence Tao is. Just a few words of \n",
      "introduction: He participated at the IMO for   the first time when he was 11 years old and he \n",
      "received a bronze medal. The next year he came   back and he received a silver medal. After that \n",
      "at the age of 13 he received a gold medal and   he was the youngest participant to receive a \n",
      "gold medal. Then he went to university and he   didn't participate at the IMO anymore. Now he \n",
      "is professor at the University of California   LA and I can say that he is definitely the \n",
      "biggest IMO star and of course one of the   most influential mathematicians of our time. \n",
      "Especially for you: Professor Terence Tao. Thank you. I'm very happy to be back here \n",
      "at the IMO. The time I had at the IMO was   one of the most fun times of my life. I still \n",
      "look back on it fondly. I hope we all had fun,   not just in the competition whether you \n",
      "get a good score or not but also in the   social activities. They always \n",
      "host a really good event here. My talk is on AI and more generally \n",
      "machine assistance in mathematics.   You've all heard about AI and how it's \n",
      "changing everything. I think earlier   today there was a talk by DeepMind on how \n",
      "there's a new product AlphaGeometry that   can answer some IMO geometry questions now. \n",
      "There will be a presentation on the AI Math   Olympia right after my talk actually \n",
      "so please stay after my talk for that. I'll be talking more about how these tools \n",
      "are beginning to change research mathematics   which is different from competition mathematics. \n",
      "Instead of having three hours to solve a problem   you take months and sometimes you don't solve the \n",
      "problem then you have to change the problem. It is   definitely not the same as math competitions \n",
      "although there's some overlap in skills. It's all very exciting and it's beginning \n",
      "to be transformative, but on the other   hand there's also a sense of continuity. We've \n",
      "actually been using computers and machines to   do mathematics for a long time and it's just \n",
      "the nature of the way in which we're doing   it is changing but it actually is following \n",
      "on a long tradition of machine assistance. So here's a question: How long have we \n",
      "been using machines to do mathematics?   The answer is thousands of years. Here's \n",
      "a machine that the Romans used to do   mathematics. The abacus is one of the early \n",
      "machines. There are even some earlier ones. That's kind of boring. That's not a really \n",
      "smart machine. What about computers? How   long have we been using computers to \n",
      "do mathematics? That's about 300-400   years. I think that's a bit weird \n",
      "because our modern computers - we   didn't have the electronic computers until \n",
      "the 1930s and 1940s. But computers weren't   always electronic. Before that they were \n",
      "mechanical and before that they were human. The computer was actually a job \n",
      "profession - someone who computes.   Here is a cluster of computers during World War \n",
      "II to compute ballistics and other things. They   had a whole cluster of computers who were mostly \n",
      "girls because the men were fighting the war. With   adding machines and it was basically - there \n",
      "were programmers who basically just told the   girls what to do. The basic unit of computational \n",
      "power at the time was not the CPU, it was the   kilgirl - how much computation you can do with \n",
      "1,000 girls working like this for one hour. We've been using computers \n",
      "actually even before that,   since the 1700s or even earlier. The most \n",
      "basic use of computers in those times was   to build tables. You may have heard of \n",
      "the logarithm tables of Napier. If you   wanted to compute sines and cosines and so \n",
      "forth, you used a computer to generate the   tables. When I was still in high school we \n",
      "still in our curriculum learned how to use   these tables that were just being phased out. Of \n",
      "course now we had calculators and now computers. We still use tables today. In mathematical \n",
      "research we rely on tables - we call them   databases now but they're still the same thing. \n",
      "There are many important results in mathematics   that were first discovered through tables. In \n",
      "number theory one of the most fundamental results   is called the prime number theorem. It tells you \n",
      "roughly how many primes there are up to a large   number X and it was discovered by Legendre and \n",
      "Gauss. They couldn't prove it but they conjectured   it to be true because Gauss and others they \n",
      "basically had computers - in fact Gauss himself   was a bit of a computer - to compute tables of \n",
      "the first million primes and try to find patterns. A couple centuries later, there's another really \n",
      "important conjecture. The prime number theorem was   eventually proven in 1907 or so. But there's \n",
      "another really central problem in number theory   called the Birch and Swinnerton-Dyer conjecture \n",
      "which I won't talk about here but it was also   first discovered by looking at lots of tables - \n",
      "this time tables of data about elliptic curves. A table that lots of mathematicians use now \n",
      "including myself is something called the Online   Encyclopedia of Integer Sequences. Maybe you've \n",
      "encountered it yourself. You may recognize many   integer sequences just from memory like if I tell \n",
      "the sequence 1, 1, 2, 3, 5, 8, 13 - you know that   is the Fibonacci sequence. The OEIS is a database \n",
      "of hundreds of thousands of sequences like this. Many times when a mathematician is working \n",
      "on a research problem there is some natural   sequence of numbers associated. Maybe there's \n",
      "a sequence of spaces depending on n and you   compute the dimension or how many or the \n",
      "cardinality of a set or something. You can   compute the first five or six or 10 of these \n",
      "numbers, put it in, and then compare it to   the OEIS. If you're lucky this sequence has \n",
      "already been put there by somebody else who   discovered it from a completely different \n",
      "source, coming from studying some other   mathematical problem. That really gives you \n",
      "a big clue that there's a connection between   two problems and many promising productive \n",
      "research projects have come up that way. Tables are one of the earliest ways we've \n",
      "been using computers. The most famous when   you think of using computers to do mathematics \n",
      "you think of number crunching. The formal name   for this is scientific computation. You want to \n",
      "do a very big calculation and you just do lots   and lots of arithmetic - you source it out to a \n",
      "computer. We've been doing that since the 1920s. Maybe the first person to really do scientific \n",
      "computation was Hendrik Lorentz. He was tasked by   the Dutch to figure out what's going to happen \n",
      "- they wanted to build a really giant dyke   and they wanted to know what happened to \n",
      "the water flow and so they had to model   some fluid equations. He used a whole \n",
      "bunch of human computers actually to   work this out. He had to invent floating \n",
      "point arithmetic to do this. He realized   that if you wanted to get a lot of people \n",
      "to do a lot of calculations very quickly   you should represent lots of numbers of \n",
      "different magnitudes as floating points. We now use computers to model all kinds of things. \n",
      "If you're solving lots of linear equations or   partial differential equations or want to do \n",
      "some combinatorial calculations, you can also   solve algebra problems. In principle, many of \n",
      "the geometry questions you see at olympiads can   be solved in principle by scientific computation. \n",
      "There are these algebra packages that can solve -   you can turn any geometry problem say involving 10 \n",
      "points and some lines and circles into a system of   equations of 20 real variables in some 20 unknowns \n",
      "and just whack it into Sage or Maple or something. Unfortunately, once it gets beyond a certain \n",
      "size the complexity becomes exponential or   even double exponential. So until recently \n",
      "it was not really feasible to just brute   force these problems with just standard \n",
      "computer algebra packages but now with   AI assistance maybe it's more promising. \n",
      "You heard a talk about that this morning. Another type of scientific computation that \n",
      "has become quite powerful is what are called   SAT solvers - satisfiability solvers. These \n",
      "are meant to solve kind of logic puzzles.   Like if you have 10 statements that are true \n",
      "or false - maybe a thousand statements that   are true or false - and you know that maybe \n",
      "if the third statement is true and the sixth   statement is true then the seventh statement \n",
      "must be false. If you're given a whole bunch   of constraints like that, a SAT solver \n",
      "will try to take all this information   and conclude - can you prove a certain \n",
      "combination of these sentences or not? There's a more fancy version \n",
      "of a SAT solver called an SMT   solver - satisfiability modulo theories \n",
      "- where you also have some variables x,   y and z and you assume some laws like maybe \n",
      "there's an addition operation and addition   is commutative and associative. You plug in \n",
      "these laws as well as some other facts and   you try to just brute force - can you deduce some \n",
      "conclusion out of some finite set of hypotheses? Those are quite powerful but unfortunately \n",
      "they also don't scale well at all. Again   the time complexity to solve them grows \n",
      "exponentially and so once you get past   a thousand or so propositions it becomes really \n",
      "hard for these solvers to run in any reasonable   amount of time. But they can actually solve \n",
      "some problems. One recent success for example   here's a problem that probably will only \n",
      "ever be solved by computer - this will   not be possible to solve by a human I think \n",
      "unassisted. It considers what was called the   Pythagorean triple problem which was unsolved \n",
      "until this big computer SAT solver calculation. The question is: You take the natural numbers and \n",
      "you color them two colors red or blue. Is it true   that no matter how you color the natural numbers, \n",
      "one of the colors must contain a Pythagorean   triple - three numbers which form the \n",
      "sides of a right triangle like 3, 4, 5? This was not known to be true. We have no \n",
      "sort of human proof of this. But we have   a computer proof. It is now known that \n",
      "in fact you don't need all the natural   numbers - you just need to go up to \n",
      "7,824. No matter how many ways you   can color 7,824 into two color classes, one \n",
      "of them will contain a Pythagorean triple. Now there's 2 to the 7824 such classes - you \n",
      "can't do it by brute force. So you have to   be somewhat clever. But it is possible. Once \n",
      "you have 7,825, you must have a Pythagorean   triple. There was an example of 7,824 where \n",
      "there's no Pythagorean triple in either class. That's provable. It actually I think it was \n",
      "the world's longest proof ever at the time. I   think now it's the second longest \n",
      "proof. The proof required a few   years of computation and it generated \n",
      "a proof certificate. The actual proof   is 200 terabytes long although it's since \n",
      "been compressed to a mere 86 gigabytes. This is one way in which we can use \n",
      "computers just to do enormous case analysis. That's sort of a fairly obvious way to use \n",
      "computers. But in recent years we've begun   to use computers in more creative ways. There are \n",
      "three ways in which computers are being used to do   mathematics which I think are really exciting, \n",
      "particularly when they are combined with each   other and with more classical databases, tables, \n",
      "and symbolic computation/scientific computation. First of all, we're using machine learning \n",
      "neural networks to discover new connections   and find out ways in which different types \n",
      "of mathematics are correlated in ways that   you would not see as a human or \n",
      "are unlikely to see as a human. Most splashy are the large language models which \n",
      "are in some sense very large versions of machine   learning algorithms, which can take natural \n",
      "language - like ChatGPT and Claude and so   forth - and they can sometimes generate possible \n",
      "approaches to problems which sometimes work,   sometimes don't. You'll see more examples \n",
      "of this actually in the talk after mine. There's also another technology which is just \n",
      "becoming usable by the everyday mathematician   which are called formal proof assistants. \n",
      "These are languages - so you know languages,   computer languages you use to write executable \n",
      "code, programs that do things - formal proof   assistants are languages that you write to check \n",
      "things, to check whether a certain argument   is actually true and actually gives you the \n",
      "conclusion from the data. These have been fairly   annoying to use until very recently and they're \n",
      "becoming now somewhat easier to use and they are   facilitating a lot of interesting math projects \n",
      "that wouldn't have been possible without these   proof assistants. They will combine very well in \n",
      "the future with the other tools I described here. So I want to talk about these more \n",
      "modern ways to use machines and   computers to do mathematics. I think \n",
      "I'll start with proof assistants. The first really computer-assisted proof maybe in \n",
      "history was the proof of the Four Color Theorem -   every planar map can be colored using only four \n",
      "colors. That was proven in 1976. This was before   proof assistants - it wouldn't really be called a \n",
      "computer proof nowadays. It was a proof which was   a massive computation, like half of which was done \n",
      "by computer and half of which was done by humans. The way they proved the Four Color Theorem \n",
      "is that you basically induct on the number   of countries. You show that if you have a massive \n",
      "map, there's some subgraph of countries - there   was a produced list of about 1,000-2,000 \n",
      "special subgraphs - and every big graph of   countries had to contain in some sense one of \n",
      "these subgraphs. That was one thing they had   to check. Then they had to check that every time \n",
      "you had a subgraph you could replace that subgraph   with something simpler and if you could four-color \n",
      "the simpler thing you could color the main thing. They had to check these properties - I \n",
      "think they're called dischargeability   and reducibility - for each of these 10,000 \n",
      "or so subgraphs. I think one of these tasks   they could do by computer although this was an \n",
      "early computer - I think they had to enter in   each graph by hand into this one program and \n",
      "check it. The other task was actually done by   a human computer - one of the daughters \n",
      "of one of the authors actually had to   spend hours and hours just manually checking \n",
      "this reducibility thing. It was very tedious. The process was not perfect - there were lots \n",
      "of little mistakes and they had to update the   table. So it was not by modern standards \n",
      "a computer proof or computer-verifiable   proof. That only came much later in the '90s \n",
      "where there was a simpler proof using a mere   700 or so graphs. But now all the things that \n",
      "need to be checked - there was a very precise   well-defined list of properties and you could \n",
      "write code in your favorite computer language,   C or Python or something, and you \n",
      "can check it in a couple pages and   a couple hundred lines of code in a \n",
      "few minutes with a modern computer. To actually check that is completely - \n",
      "to write a proof that goes all the way   down to the axioms of mathematics \n",
      "- that was done in 2005 using a   proof assistant language called Coq. \n",
      "I think it's now renamed to Rooster. That was one of the first proofs and you \n",
      "see there's a huge gap between sort of   when the proof first appeared and then when we \n",
      "actually could completely verify it by computer. Another famous example is the Kepler conjecture \n",
      "for sphere packing. This is a really old   conjecture - Kepler from the 17th century. It's \n",
      "very easy to state: You take a whole bunch of unit   spheres and you want to cover three-dimensional \n",
      "space as efficiently as possible. There's sort of an obvious way to try to \n",
      "pack spheres - it's a triangular packing.   Like the way you pack oranges at a grocery \n",
      "store. There's also a dual packing called   the cubic packing which has the same \n",
      "density. There's a density of about   74% which is the obvious packing and the \n",
      "question is - is that the best possible? This turns out to be a surprisingly \n",
      "hard problem. In two dimensions,   the hexagonal packing is not too hard to show \n",
      "it's the best. Only very recently in 8 and 24   dimensions do we know the answer - great \n",
      "work of Viazovska, maybe she talked about   it yesterday. But three was the only other case \n",
      "that we know except for one which is trivial. Surprisingly difficult to prove. Again there \n",
      "was no completely human-readable proof of   this conjecture. There is a strategy - \n",
      "so of course the problem is that there   are infinitely many of these spheres \n",
      "and the density is an asymptotic thing   so it's not a priori a finite problem \n",
      "that you can just throw at a computer. But you can try to reduce it to a finite \n",
      "problem. There's a strategy proposed by   Toth in the 50s. Every time you have a packing, \n",
      "it subdivides space into these polyhedra called   Voronoi regions. The Voronoi polytope of \n",
      "a sphere is just all the points which are   closer to the center of that sphere than \n",
      "to all the other spheres. So you can sort   of split up space into all these polytopes and \n",
      "these polytopes have certain volumes. You can   also count their faces and the surface areas and \n",
      "so forth and so they all have these statistics. The volumes - like the packing density \n",
      "is very closely related to sort of the   average volume of these regions. So if you \n",
      "can say something about kind of how these   volumes of these polytopes behave on average,   then you could get at least maybe some upper \n",
      "bound onto how efficient these packings can be. You can try to make relations between \n",
      "these polytopes - like if one polytope   is very big maybe it forces the nearby ones \n",
      "to be very small and so maybe you can try to   find some inequalities connecting the \n",
      "volume of one polytope to another. So   maybe you should just collect lots and \n",
      "lots of these inequalities and then do   some linear programming or something \n",
      "and hopefully you can just derive the   right bound of this magic π/√18 which is the \n",
      "right density from all these inequalities. People tried this. There were many attempts - some   even claimed success. But none have \n",
      "been accepted as actual proofs. The problem was eventually solved first by Thomas \n",
      "Hales and his co-authors. He did basically the   same strategy but with lots of technical tweaks. \n",
      "He changed the cells from Voronoi cells to   slightly more fancy cells. Instead of taking \n",
      "the volume he invented this called the score   that he assigned to each of these - it's a volume \n",
      "plus or minus lots of little ad hoc adjustments. But again with the aim of trying to \n",
      "create all these linear inequalities   between these different scores and to eventually   get upper bounds of the density and to \n",
      "hopefully hit exactly the optimal density. It's a very flexible method - actually it's too \n",
      "flexible because there are too many things you   can try. There are so many ways you can set up \n",
      "the score and so forth. So there's a quote here:   \"Sam Ferguson realized that every time he \n",
      "encountered problems in trying to minimize   his functional and so forth he could \n",
      "just change the score and try it again.\" But then all the things that they checked \n",
      "already they had to redo. And so the scoring   function became more and more complicated. \n",
      "You know they worked on this for almost a   decade I think. It became more and more \n",
      "complicated but each change we cut months   through years from my work. This incessant \n",
      "fiddling was unpopular with our colleagues.   Every time I presented my work in progress \n",
      "at a conference I was minimizing a different   function. Even worse the function was \n",
      "mildly incompatible with what I did in   earlier papers and this required going \n",
      "back and patching the earlier papers. But eventually they did it. In 1998 they announced   that they had finally found a score \n",
      "which obeyed a whole bunch of linear   inequalities in 150 variables which \n",
      "they did minimize and got their thing. Initially they did not plan to make this a \n",
      "computer-assisted proof but as the project became   more and more complicated it was inevitable \n",
      "they had to use more and more computers. The proof was enormous by the standards of 1998. \n",
      "It was 250 pages of notes and three gigabytes of   computer programs and data. It actually had a \n",
      "very tough time getting refereed. It got sent   to the top journal Annals of Mathematics \n",
      "and it took four years to referee with a   panel of 12 referees. At the end they said \n",
      "they were 99% certain of the correctness of   the proof but they could not certify the \n",
      "correctness of the computer calculations. They did a very unusual thing actually \n",
      "- they published the paper with a little   caveat from the editors saying this. \n",
      "They have since removed that caveat   actually. At the time there was a lot more \n",
      "controversy as to whether a computer-assisted   proof qualified as an actual proof. Now I \n",
      "think we are much more comfortable with it. But even after it was published there was doubt \n",
      "about whether it was really a proof. So this   was maybe the first major high-profile problem \n",
      "where there was really a big incentive to really   formalize this completely all the way down to \n",
      "first principles in a formal proof language. So Hales in fact created a language \n",
      "- well a modification of existing   languages to do this. He called it the \n",
      "Flyspeck project. He estimated it would   take 20 years to formalize his proof but \n",
      "actually with the help of 21 collaborators   he actually finished in a mere 12 \n",
      "years. It finally appeared in 2014. So we now have sort of complete confidence in this   particular result but it was \n",
      "quite a painful thing to do. Moving now to the last few years - we've \n",
      "now figured out sort of a better workflow   for how to formalize. It's still \n",
      "tedious but it is getting better. Peter Scholze, who is a very prominent young \n",
      "mathematician - Fields Medalist for instance -   he's famous for many many things but he created \n",
      "this amazingly promising area of mathematics.   It's called condensed mathematics. It deploys \n",
      "the power of algebra, category theory and all   the tools from algebra to apply to functional \n",
      "analysis - the theory of function spaces like   Banach spaces and so forth which in analysis has \n",
      "really been resistant to the methods of algebra. But this area of mathematics in principle \n",
      "could allow one to solve questions in at   least functional analysis - certain \n",
      "types of questions - with algebraic   methods. So he set up this whole category \n",
      "of these things called condensed abelian   groups and condensed vector spaces. I won't \n",
      "take long to explain what condensed means. His thesis is that all our categories of function \n",
      "spaces that we learn in our graduate classes are   incorrect or they're not the natural ones \n",
      "- there are ones with better properties. So he set up this theory but there was this \n",
      "one very important vanishing theorem which   he needed to prove. I stated it here but I'm \n",
      "not going to explain what any of these words   mean or symbols mean. But there was a very \n",
      "technical vanishing of a certain category   theoretic group he needed to compute. \n",
      "Without this the whole theory doesn't   have any interesting consequences. This \n",
      "was sort of the foundation of this theory. So he wrote a blog post about this result. He \n",
      "said he spent a whole year getting obsessed with   the proof of this theorem, going almost crazy over \n",
      "it. In the end we were able to get an argument put   down on paper but no one has dared to look at the \n",
      "details of this so I still have lingering doubts. \"With this theorem the hope that this condensed \n",
      "formalism can be fruitfully applied to functional   analysis stands or falls. This theorem is of the \n",
      "utmost foundational importance so being 99.9% sure   is not enough.\" He said he was happy to see many \n",
      "study groups on condensed mathematics throughout   the world but they all stopped short of the \n",
      "proof of this theorem. \"This proof is not much   fun.\" So he says \"This may be my most important \n",
      "result to date - better be sure it's correct.\" So he was also very incentivized \n",
      "to formalize this theorem now in a   more modern proof assistant language called Lean. Lean is a language that has been developed \n",
      "quite a lot in recent years. It comes with   a crowdsourced effort to develop this massive \n",
      "math library. Rather than deriving everything   from the axioms of mathematics which becomes \n",
      "very tedious the more advanced you go - and   this type of mathematics is very advanced - this \n",
      "central math library in Lean has already proved   lots of intermediate results like the type of \n",
      "things you would see in say undergraduate math   courses like calculus or basic theorems \n",
      "of group theory or topology and so forth. These have already been formalized and \n",
      "so you have a standing base - you're not   starting from the axioms, you're \n",
      "starting from roughly a graduate   level math education. Still a big gap \n",
      "to where you need to go but it helps. But in order to formalize this they had to add \n",
      "many extra things. The math library was not   complete - it's still not complete. There's lots \n",
      "of areas of mathematics like homological algebra,   sheaf theory, topos theory that \n",
      "needed to be added to the library. But in a mere 18 months they were \n",
      "able to formalize this theorem.   The proof was basically correct. There were \n",
      "some minor technical issues but nothing really   major was discovered. They found some nice \n",
      "simplifications. There were some technical   steps that were just too hard to formalize and \n",
      "so they were forced to find some shortcuts. But actually the value of this \n",
      "project was more indirect. Firstly,   they greatly added to Lean's math \n",
      "library. So now this math library   can handle lots of abstract algebra to a \n",
      "much greater extent than it could before. But also there were other supporting \n",
      "software that got set up that future   formalization projects have started \n",
      "using, including some that I did. For example, one tool that was set \n",
      "up in the course of this project was   what's called a blueprint. Taking a huge 50 \n",
      "page proof and trying to directly formalize   it is really painful. You have to \n",
      "keep the whole proof in your head. But what we've realized is the right workflow \n",
      "is that you take a big proof and you first write   what's called a blueprint which sort of breaks \n",
      "up this proof into like hundreds of tiny little   steps. Each step you can formalize separately \n",
      "and then you just put them all together. So you try to break up a huge argument into lots   of little pieces. You write that first \n",
      "and then different people in your team   can formalize different parts of \n",
      "different steps of your argument. So they also as a byproduct of this \n",
      "formalization also produced this very   nice blueprint. This is probably the \n",
      "- if you actually want to read the   proof as a human - the blueprint is \n",
      "probably the best place to go to now. Another spin-off of this - so there's now also \n",
      "this formal proof which is tens of thousands   of lines long but now there are efforts to try \n",
      "to convert that back to a human readable proof. So another thing that's been developed is \n",
      "that there are now tools - you can take a   proof that's been written in say this language \n",
      "Lean - like here's an example where there's a   proof written of a topological problem and \n",
      "they converted it back to a human readable   proof. So all this text here is a proof that \n",
      "is computer generated from a formal proof. It looks like a human proof. It uses the same sort \n",
      "of math language but it's much more interactive.   You can click on any location - I can't do it \n",
      "because it's a static PDF - but you can click   on any location here and it will tell you wherever \n",
      "you are what the hypothesis is, what you're trying   to prove, what the variables are. If there's \n",
      "a step that is too short you can expand it and   it will explain where it came from and you can \n",
      "go all the way down to the axioms if you want. I think this is great. I think in the \n",
      "future textbooks will be written in   this interactive style. You formalize them first   and then you can have much more \n",
      "interactive textbooks than currently. Inspired by this, I myself started a \n",
      "project to formalize - so I recently   last year I solved a problem in combinatorics \n",
      "with several people including Tim Gowers who's   here in the audience. It's a problem in \n",
      "combinatorics - it's not super important   what the problem is. There's a subset of Z mod \n",
      "2 to the n, like what's called the Hamming cube,   and it obeys a property called small \n",
      "doubling. Then there's a certain limit   to how big it can be. But it doesn't \n",
      "really matter what the statement is. We proved it. The proof is about 33 pages. We \n",
      "formalized it in relatively record time - actually   probably still the fastest formalized actual \n",
      "research paper. In 3 weeks, in a group project of   about 20 people using all this blueprint machinery \n",
      "that had been developed in Scholze's project. It makes the task of proving things much \n",
      "more open and collaborative. You get all   these nice visualizations. As I \n",
      "said, the first thing you do is   that you take your big theorem and you \n",
      "break it up into lots of little pieces. The theorem that we have is we call it PFR - \n",
      "won't explain why. That corresponds to this   little bubble at the bottom of this graph \n",
      "here and then we introduce all these other   statements. The proof of PFR has to depend \n",
      "on several other previous statements. These   ones depend on previous statements as \n",
      "well. So there's this dependency graph   and they have different colors depending \n",
      "on whether you've formalized them or not. A green bubble is a statement that you've already \n",
      "formally proven in your language. A blue bubble   is one that hasn't yet been formalized but it's \n",
      "ready to be formalized - like all the definitions   are in place, someone needs to actually go \n",
      "ahead and do it. A white bubble - even the   statement hasn't been formalized yet, \n",
      "someone has to write in the statement. So you get this tree of tasks. The \n",
      "beauty of this project is that you   can get all these people to collaborate \n",
      "on these different pieces of this graph   independently. Every little bubble \n",
      "corresponds to some statement and   you don't need to understand the whole proof \n",
      "in order to just work on your little piece. This was a problem in combinatorics \n",
      "but the people who contributed - there   were people from probability, there were \n",
      "people who were not even mathematicians,   they were computer programmers but they were \n",
      "just very good at sort of doing these little   mini puzzle type things. Everyone just sort of \n",
      "picked one bubble that they think they could   do and they did it. In three weeks we did the \n",
      "whole thing. It was a really exciting project. In mathematics we don't normally collaborate \n",
      "with this many people. Maybe five people   is the most I've normally seen because when you \n",
      "collaborate on a big project you have to trust   that everyone's math is correct and past a certain \n",
      "size this is just not feasible. But with a project   like this the Lean compiler automatically checks \n",
      "- you cannot upload anything that doesn't compile,   it will get rejected. So you can collaborate \n",
      "with people that you never met before. I met a lot of people actually - wrote   a lot of letters of recommendation \n",
      "actually coming out of this project. This is an example - like this is one \n",
      "little piece of the proof. This is what   a proof looks like in Lean. It's \n",
      "not exactly - I mean if you know   the language it's human readable \n",
      "but it looks a little bit unusual. It really sort of decouples the task \n",
      "of proving things into many different   sort of disjoint skills. You can have \n",
      "some people who see the big picture and   organize things into little pieces \n",
      "and then you have people who don't   necessarily know all the mathematics but \n",
      "can just work on little pieces at a time. I think this will be a more and more common \n",
      "way of doing mathematics going forward. It   still is painful to do - like the tools \n",
      "are not really - they're getting better   and user-friendlier but you still need \n",
      "to have some expertise in programming.   I would say it takes maybe 10 times longer to \n",
      "formalize a proof than to write it by hand. On the other hand if you want to \n",
      "change a proof - so for example   there was a 12 that showed up in this theorem, we \n",
      "later improved this 12 to an 11. We got a slightly   stronger theorem. Normally if you do that you have \n",
      "to rewrite the whole proof or like you could maybe   cut and paste 12 to 11 but then you have to check \n",
      "you didn't make any mistakes when you did that. But actually when we formalized this then \n",
      "we got the improvement - it only took a   few days to change the theorem to the 11. \n",
      "We just changed the 12 to 11 somewhere and   then the compiler complained like five \n",
      "different places now is certain this   very specific part is not working and \n",
      "we could just do some targeted fixes. So in fact for some specific \n",
      "types of doing mathematics   already the formal approach is actually faster. Now there are actually quite a few big proof \n",
      "formalization projects going on right now.   The biggest is Kevin Buzzard's project - he's just \n",
      "got a big grant to formalize Fermat's Last Theorem   in Lean. He says it will take five years to do \n",
      "the most important parts of this proof. He doesn't   claim to do the whole thing in five years but the \n",
      "interesting part is already on the way actually. So that is formal proof assistants. \n",
      "I'll talk about machine learning. Machine learning - these are using \n",
      "neural networks to predict answers   to various questions that you can use in \n",
      "many ways. I think I'll skip the first   way I was discussing which is to use \n",
      "neural networks to guess solutions to   differential equations which is a very \n",
      "exciting new tool in PDEs but I will skip it. I'll talk about another application of \n",
      "machine learning to knot theory. Knot   theory is quite a fun area of mathematics. \n",
      "It's an interesting area that brings together   many different fields of mathematics and \n",
      "they don't really talk to each other. A knot is just a loop of string or a curve \n",
      "really in space that is closed. Two knots are   equivalent if there's some way to continuously \n",
      "deform one knot to another in a way in which   you're not allowed to cross the string - \n",
      "the string is not allowed to cross itself. The basic questions in knot theory \n",
      "are: When are two knots equivalent?   If I give you two knots is there \n",
      "some way to turn one into the other? The way you approach this question normally \n",
      "is that you develop these things called knot   invariants. These are various numbers, \n",
      "sometimes also polynomials that you   can attach to a knot and these numbers don't \n",
      "change no matter how you continuously deform   the knot. So if two knots have different \n",
      "invariants they cannot be equivalent. There are many many types of knot \n",
      "invariants. There's something   called the signature which counts - you \n",
      "flatten the knot and you count crossings,   whether the crossings go over or under and \n",
      "you create a certain matrix and so forth   and you can get a certain integer called the \n",
      "signature. That is one type of knot invariant. There are some famous polynomials called the \n",
      "Jones polynomial and the Alexander polynomial   which are connected to many areas of \n",
      "mathematics but I won't talk about that. Then there are these things called \n",
      "hyperbolic invariants which come from   geometry. You can take the complement \n",
      "of the knot and that is actually what's   called a hyperbolic space. It comes \n",
      "with a certain geometric structure,   has a notion of distance and you can compute \n",
      "its volume and some other invariants. These are invariants that are real \n",
      "or complex numbers. So every knot   comes with some combinatorial invariants \n",
      "like signatures and it comes with these   geometric invariants like these hyperbolic \n",
      "invariants. Here is a whole list of knots   with various hyperbolic invariants - there's \n",
      "something called the hyperbolic volume and   the homological cusp shape and so forth. \n",
      "These are real or complex numbers but no   one knew of any link between these two. \n",
      "There were these two separate ways to   create statistics of knots and they didn't \n",
      "- there was no connection between them. It was only very recently that people \n",
      "started using machine learning to attack   this problem. They created databases \n",
      "of millions of knots which actually   was already a slightly nontrivial task and \n",
      "they trained a neural network on this. They   found that after training the neural network \n",
      "you could give it all the hyperbolic geometry   invariants and like 90% of the time it will \n",
      "predict - it will guess the right signature. So it created this black box and it will \n",
      "tell you how the signature was somehow hidden   somewhere in these geometric invariants but it \n",
      "didn't tell you how - it was this black box. But that's still useful because once you have this \n",
      "black box you can just play with it. So what they   did next is actually very simple analysis - this \n",
      "is what's called saliency analysis. What this   black box does - it takes about 20 different \n",
      "inputs, one for each hyperbolic invariant,   and which is one output - the signature. So \n",
      "once you have this black box you can just tweak   each input. You just say what if I change one \n",
      "input, how likely is it to change the output? Of the 20 inputs that they found, they \n",
      "found that only three of them actually   played a really major role in the output. \n",
      "The other 17 were barely relevant and it   wasn't the three that they expected \n",
      "actually. They expected the volume   for example to be very important and the \n",
      "volume turns out to be almost irrelevant. There were three - something called longitudinal \n",
      "translation and the real and complex parts of   meridional translation - there were these three \n",
      "invariants that were the most important. So once   they identified the ones that were most important, \n",
      "they could just plot directly the signature   against those three particular inputs and then \n",
      "they could eyeball - rather than use neural   network they use the human network - to then \n",
      "see oh okay there's some obvious patterns here. By staring at these graphs they could \n",
      "actually make conjectures as to what was   actually going on. They made a conjecture \n",
      "based on this which turned out to be wrong   actually. But they actually used the neural \n",
      "network to show that it was wrong. But then   the way that it failed, they could correct \n",
      "it and they found a corrected version of   the conjecture which actually did explain \n",
      "this phenomenon. Then once they found the   right statement they were able to prove \n",
      "it. So they actually have a theoretical   explanation of why the signature is so closely \n",
      "related to these particular statistics. This I think is a way in which machine \n",
      "learning is being increasingly used in   mathematics. It doesn't directly solve \n",
      "the problem for you but it gives you all   these really useful hints as to where the \n",
      "connections are and where to look at them,   but you still need the human to \n",
      "actually make the connections. And then finally we have the large language models \n",
      "which are the most splashy and have made the most   news. Neural networks have been around for 20 \n",
      "years but large language models have also been   around for 5 or so years but they've only become \n",
      "sort of human level in output very recently. You've all probably heard of GPT-4. This is \n",
      "ChatGPT's current model. Very famously when   GPT-4 came out there was a paper describing \n",
      "its capabilities and they fed it basically a   question from the 2022 IMO. It's a slightly \n",
      "simplified version - if you studied the 2022   IMO you'll probably notice it's not exactly \n",
      "the same form but it's a simplified form. For this particular question actually you give \n",
      "it the question and it actually gives a complete   correct solution to this question. It actually \n",
      "solved an IMO question. Unfortunately this is   an extremely cherry-picked example. I think out of \n",
      "the hundreds of IMO level questions they tested it   on, they had a success rate of about 1%. So this \n",
      "particular problem they were able to solve, and   they had to format the problem in the right way to \n",
      "get the solution, but still this is quite amazing. On the other hand the funny thing about these \n",
      "tools is that things that humans find difficult,   AI can do very easily sometimes, \n",
      "but things that humans find easy   AI often struggles with. It is a very \n",
      "orthogonal way of solving problems. In the same related paper or presentation, they \n",
      "asked the same model to do a basic arithmetic   computation: 7 * 4 + 8 * 8. The model, \n",
      "which is just guessing the most likely   output based on the input, basically guessed \n",
      "the answer is 120. Then it paused and said   okay maybe I should give an explanation why \n",
      "it's 120. So they did a step by step but   when they did a step by step they actually \n",
      "arrived at the actual answer which is 92,   not the answer that they started with. So \n",
      "then if you asked \"Wait but you said that   it was 120\" and they said \"Oh that was \n",
      "a typo, sorry the correct answer is 92.\" So you know they're not solving the problem \n",
      "from first principles, they're just guessing at   each step of the output what is the most natural \n",
      "thing to say next. The amazing thing is sometimes   that works, but often it doesn't. It's still \n",
      "ongoing how to sort of make it more accurate. People are trying all kinds of things. \n",
      "You can connect these models to other   more reliable software. In fact you will see a \n",
      "presentation after where there's a large language   model connected where you don't do the computation \n",
      "yourself, you outsource it to Python in that case. But another thing you can do is that you \n",
      "can force the language model to only produce   correct answers by forcing the model to output \n",
      "in one of these proof assistant languages and   if it doesn't compile you send it back \n",
      "to the AI and the AI has to try again. Or you can try to teach it directly the \n",
      "same problem solving techniques we use   to solve IMO problems - you know, try \n",
      "simple examples, prove by contradiction,   try to actually prove step by step and so forth. So people are trying all kinds of things. It's \n",
      "still nowhere near able to solve a large majority   of say math olympiad problems, let alone math \n",
      "research problems, but we're making progress. Besides being able to actually solve \n",
      "problems directly, it's also useful   just as a muse actually. I've also used these \n",
      "models myself. I've experimented with various   problems - I had a combinatorics problem \n",
      "which I was trying a few things and they   weren't working so I as an experiment I just \n",
      "tried asking GPT \"What other techniques would   you suggest to solve this question?\" It \n",
      "gave me a list of 10 techniques of which   like 5 I'd already tried or were obviously \n",
      "not helpful. But there was one technique   I'd not tried which was to use generating \n",
      "functions for this particular question,   which once it was suggested I realized it \n",
      "was the right approach but I had missed it. So just as someone to converse with, it is \n",
      "somewhat useful. It is not great right now   but it is not completely useless. There's another type of AI assistance that's \n",
      "actually become very useful for proof assistants.   As I said, writing formal proofs is a very tedious \n",
      "task. I mean it's like any really fussy computer   language - you have to get the syntax exactly \n",
      "right. If you miss a step it doesn't compile. But there are tools - so I use something called \n",
      "GitHub Copilot where you can write down half of   a proof and it will try to guess what the next \n",
      "line is. About 20% of the time it actually   guesses something close to being correct and then \n",
      "you can just say I'll accept that and say okay. So in this case I was trying to prove this \n",
      "statement here and the lines in gray are   the ones that Copilot suggested. It turns out \n",
      "the first line is useless but the second line,   which you can't quite see, actually \n",
      "did solve this particular problem. So you still have to - you can't just accept \n",
      "the input because it won't necessarily compile.   But if you already know sort of how the \n",
      "code works it saves you a lot of time. These tools are getting better. So right \n",
      "now they can maybe - if a proof is one   line or two lines long they can fill it in \n",
      "automatically. There are now experiments to   sort of iterate an AI suggesting a proof \n",
      "and then you feed it back to the compiler   and then if it compiles wrong you send \n",
      "the error message back. We're beginning   to sort of prove things that are like 4 or 5 \n",
      "lines long - they can be done by this method. Of course a big proof is like tens of thousands \n",
      "of lines so it's nowhere near the point where you   can just instantly get your proof formalized \n",
      "immediately. But it is already a useful tool. Okay so where are we now? There are people \n",
      "who are hoping that in a few years we can   use computers to actually solve math problems \n",
      "directly. I think we are still a long way away   from that. For very narrowly focused problems \n",
      "you can sort of set up specialized AI to handle   just a very narrow band of problems. But even then \n",
      "they're not fully reliable - they can be useful. But still for the next few years at \n",
      "least, they're basically going to be   really useful assistants beyond the sort \n",
      "of brute force computational assistance   that we're already familiar with. People \n",
      "are trying all kinds of creative things. I think one direction which I find particularly \n",
      "exciting hasn't really been successful yet -   hopefully AI will become very good at generating \n",
      "good conjectures. We already saw a little example   of this with the knots where they could already \n",
      "sort of conjecture those connections between two   different statistics. So you know there's just the \n",
      "hope that you just create these enormous datasets   and feed them into an AI and they would just \n",
      "automatically generate lots of nice connections   between different mathematical objects. We don't \n",
      "really know how to do this yet partly because we   don't have these massive datasets but I think this \n",
      "is something that would eventually be possible. One thing I'm also excited about - this is a \n",
      "type of math that just doesn't exist yet. Right   now because proving theorems is such a painful \n",
      "painstaking process we prove one theorem at a time   or maybe two or three if you're efficient. But \n",
      "with AI you could imagine in the future instead of   trying to prove one problem or solve one problem, \n",
      "you take a class of 1,000 similar problems and   you say \"Okay I'm going to tell your AI try to \n",
      "solve these 1,000 problems with this technique\"   and it will report back \"Oh I could solve 35% of \n",
      "these problems with this technique. What about   this technique? I can solve this percentage \n",
      "of problems. Well if I combine them I can   do this.\" You could start exploring the space of \n",
      "problems rather than just each problem separately. This is something that you just you \n",
      "either cannot do right now or you   do over a process of decades with dozens \n",
      "and dozens of papers slowly figuring out   what you can and can't do with various \n",
      "techniques. But with these tools you   could really start doing mathematics on \n",
      "a scale which is really unprecedented. So the future is going to be really \n",
      "exciting Ithink. I mean they we will   still also be proving theorems the old \n",
      "fashioned way. In fact we'll have to   because we can't we won't be able to guide \n",
      "these AIs unless we also know how to do the   things ourselves. But we'll be able to do \n",
      "lots of things that we can't do right now. Okay I think I will stop there. So \n",
      "thank you very much. Any questions? So we are on a tight schedule but I \n",
      "was told we have time for maybe three   or so questions. So if people would raise \n",
      "hands and - there's someone over there. Thank you. Can you hear me? Thank you that was \n",
      "a beautiful talk. I particularly loved about   formalizing mathematics but one thing that \n",
      "you didn't mention was Voevodsky who left   algebraic geometry because he made a mistake \n",
      "and started formalizing homotopy type theory.   I would be interested to know if you have \n",
      "studied this and have any comments on it. Right. For Voevodsky - yeah he was \n",
      "worried about a crisis in certain   areas of mathematics including some \n",
      "that he created, that the proofs were   so abstract and sophisticated that there was no \n",
      "way to verify that they were completely true. Yeah so he proposed changing the foundation \n",
      "of mathematics to homotopy type theory,   which is more robust. Like if you change \n",
      "the underlying axioms of mathematics,   a lot of what you prove in \n",
      "this theory is still true. There are proof assistant languages that \n",
      "are based on this sort of homotopy type   theory. Lean is not actually, by design, \n",
      "because Lean wants to formalize a lot   of traditional mathematics which \n",
      "is not written in this language. I do hope in the future there will be \n",
      "multiple proof assistant languages with   different strengths and weaknesses. One thing \n",
      "that we don't have right now is automatic ways   to translate a proof in one language to another. \n",
      "That's actually one place where AI I think will   be very useful. Once we have that then we can \n",
      "- if you have a different philosophy of your   foundation of mathematics we could just hopefully \n",
      "translate a proof that's been formalized in one   language to another and then everyone will \n",
      "be convinced, including Voevodsky hopefully. Yeah so I mean there are multiple \n",
      "approaches to formalizing mathematics   and we shouldn't just certainly fix \n",
      "on one particular standard just yet. Well I - okay. All right so a random process. \n",
      "Okay, well that will not be quite relevant   to the topic of the talk, but I was recently \n",
      "applying for PhDs and the advice I was given   by the professors was basically along the lines of \n",
      "\"the longer the better\". So it seems like there's   kind of a general agreement that mathematicians \n",
      "need somehow to grow up to big ideas. So from   that perspective, how do you think about your \n",
      "decision about going to university at such a   young age? Did you think - how it influenced \n",
      "you as a mathematician and as a human being? Well I was - yeah I had some very good advisers,   both as a high school and undergraduate \n",
      "and graduate level. I mean I don't think   it's a race. I mean you go to university \n",
      "when you're ready to go. You know you   shouldn't go just because you were told \n",
      "you need X years or something to do this. I think it's different for different people. You \n",
      "know I mean - it was very important for me. I mean   I went to undergraduate when I was 13 but it was \n",
      "at a university that was very close to where I   lived so I lived with my parents and they drove \n",
      "quite a lot actually to the university for all   my classes. If I didn't have that I don't \n",
      "think I would have had a good experience. So it really depends. I mean okay I did my \n",
      "university at a very young age - doesn't   mean everybody should do that. Yeah it's - \n",
      "there's no single answer to this question. Thank you. Okay so another kind of more general \n",
      "question: Given that you've contributed   to truly myriad mathematical fields, \n",
      "how do you go about choosing your next   research topic and problem you want to \n",
      "solve? And also what's your Erdős number? Okay well my Erdős number is two, that's easy. \n",
      "But I actually don't know - I mean it - I mean   early on in my career you know I had advisers who \n",
      "suggested problems to me. Nowadays it often just   comes by serendipity. I mean math is a very \n",
      "social activity. You go to lots of events. So I mean after this I'm going to a math \n",
      "conference in Edinburgh and I'm going to talk   to a lot of people in actually an area connected \n",
      "to this PFR conjecture thing actually. Likely I'll   have some interesting math conversations and \n",
      "maybe some research questions come out of it. I do have some long-term projects that I \n",
      "would like to - you know something that   I'd like to solve. But increasingly I find \n",
      "that it's the questions that just come up by   conversation with other mathematicians \n",
      "that - yeah so you know I didn't know   I'd be working - be talking so much about AI \n",
      "actually until about two years ago for instance. Yeah I think people should be sort of - you know \n",
      "the future is going to require more flexibility.   I mean there will still be people who specialize \n",
      "in one topic and just one topic and be the world   expert in X, but increasingly I think there'll \n",
      "be more and more people who will move around over   time and just find interesting new mathematics \n",
      "every few years just by talking to other people. Okay. I think we have to move \n",
      "on actually. So the next speaker   is Simon Coyle from XTX who will \n",
      "talk about the AI Math Olympiad.\n",
      "'Hello everyone. Hello everyone. Some of you dear contestants\\xa0\\nare very young so perhaps you do not know who\\xa0\\xa0 Professor Terence Tao is. Just a few words of\\xa0\\nintroduction: He participated at the IMO for\\xa0\\xa0 the first time when he was 11 years old and he\\xa0\\nreceived a bronze medal. The next year he came\\xa0\\xa0 back and he received a silver medal. After that\\xa0\\nat the age of 13 he received a gold medal and\\xa0\\xa0 he was the youngest participant to receive a\\xa0\\ngold medal. Then he went to university and he\\xa0\\xa0 didn\\'t participate at the IMO anymore. Now he\\xa0\\nis professor at the University of California\\xa0\\xa0 LA and I can say that he is definitely the\\xa0\\nbiggest IMO star and of course one of the\\xa0\\xa0 most influential mathematicians of our time.\\xa0\\nEspecially for you: Professor Terence Tao. Thank you. I\\'m very happy to be back here\\xa0\\nat the IMO. The time I had at the IMO was\\xa0\\xa0 one of the most fun times of my life. I still\\xa0\\nlook back on it fondly. I hope we all had fun,\\xa0\\xa0 not just in the competition whether you\\xa0\\nget a good score or not but also in the\\xa0\\xa0 social activities. They always\\xa0\\nhost a really good event here. My talk is on AI and more generally\\xa0\\nmachine assistance in mathematics.\\xa0\\xa0 You\\'ve all heard about AI and how it\\'s\\xa0\\nchanging everything. I think earlier\\xa0\\xa0 today there was a talk by DeepMind on how\\xa0\\nthere\\'s a new product AlphaGeometry that\\xa0\\xa0 can answer some IMO geometry questions now.\\xa0\\nThere will be a presentation on the AI Math\\xa0\\xa0 Olympia right after my talk actually\\xa0\\nso please stay after my talk for that. I\\'ll be talking more about how these tools\\xa0\\nare beginning to change research mathematics\\xa0\\xa0 which is different from competition mathematics.\\xa0\\nInstead of having three hours to solve a problem\\xa0\\xa0 you take months and sometimes you don\\'t solve the\\xa0\\nproblem then you have to change the problem. It is\\xa0\\xa0 definitely not the same as math competitions\\xa0\\nalthough there\\'s some overlap in skills. It\\'s all very exciting and it\\'s beginning\\xa0\\nto be transformative, but on the other\\xa0\\xa0 hand there\\'s also a sense of continuity. We\\'ve\\xa0\\nactually been using computers and machines to\\xa0\\xa0 do mathematics for a long time and it\\'s just\\xa0\\nthe nature of the way in which we\\'re doing\\xa0\\xa0 it is changing but it actually is following\\xa0\\non a long tradition of machine assistance. So here\\'s a question: How long have we\\xa0\\nbeen using machines to do mathematics?\\xa0\\xa0 The answer is thousands of years. Here\\'s\\xa0\\na machine that the Romans used to do\\xa0\\xa0 mathematics. The abacus is one of the early\\xa0\\nmachines. There are even some earlier ones. That\\'s kind of boring. That\\'s not a really\\xa0\\nsmart machine. What about computers? How\\xa0\\xa0 long have we been using computers to\\xa0\\ndo mathematics? That\\'s about 300-400\\xa0\\xa0 years. I think that\\'s a bit weird\\xa0\\nbecause our modern computers - we\\xa0\\xa0 didn\\'t have the electronic computers until\\xa0\\nthe 1930s and 1940s. But computers weren\\'t\\xa0\\xa0 always electronic. Before that they were\\xa0\\nmechanical and before that they were human. The computer was actually a job\\xa0\\nprofession - someone who computes.\\xa0\\xa0 Here is a cluster of computers during World War\\xa0\\nII to compute ballistics and other things. They\\xa0\\xa0 had a whole cluster of computers who were mostly\\xa0\\ngirls because the men were fighting the war. With\\xa0\\xa0 adding machines and it was basically - there\\xa0\\nwere programmers who basically just told the\\xa0\\xa0 girls what to do. The basic unit of computational\\xa0\\npower at the time was not the CPU, it was the\\xa0\\xa0 kilgirl - how much computation you can do with\\xa0\\n1,000 girls working like this for one hour. We\\'ve been using computers\\xa0\\nactually even before that,\\xa0\\xa0 since the 1700s or even earlier. The most\\xa0\\nbasic use of computers in those times was\\xa0\\xa0 to build tables. You may have heard of\\xa0\\nthe logarithm tables of Napier. If you\\xa0\\xa0 wanted to compute sines and cosines and so\\xa0\\nforth, you used a computer to generate the\\xa0\\xa0 tables. When I was still in high school we\\xa0\\nstill in our curriculum learned how to use\\xa0\\xa0 these tables that were just being phased out. Of\\xa0\\ncourse now we had calculators and now computers. We still use tables today. In mathematical\\xa0\\nresearch we rely on tables - we call them\\xa0\\xa0 databases now but they\\'re still the same thing.\\xa0\\nThere are many important results in mathematics\\xa0\\xa0 that were first discovered through tables. In\\xa0\\nnumber theory one of the most fundamental results\\xa0\\xa0 is called the prime number theorem. It tells you\\xa0\\nroughly how many primes there are up to a large\\xa0\\xa0 number X and it was discovered by Legendre and\\xa0\\nGauss. They couldn\\'t prove it but they conjectured\\xa0\\xa0 it to be true because Gauss and others they\\xa0\\nbasically had computers - in fact Gauss himself\\xa0\\xa0 was a bit of a computer - to compute tables of\\xa0\\nthe first million primes and try to find patterns. A couple centuries later, there\\'s another really\\xa0\\nimportant conjecture. The prime number theorem was\\xa0\\xa0 eventually proven in 1907 or so. But there\\'s\\xa0\\nanother really central problem in number theory\\xa0\\xa0 called the Birch and Swinnerton-Dyer conjecture\\xa0\\nwhich I won\\'t talk about here but it was also\\xa0\\xa0 first discovered by looking at lots of tables -\\xa0\\nthis time tables of data about elliptic curves. A table that lots of mathematicians use now\\xa0\\nincluding myself is something called the Online\\xa0\\xa0 Encyclopedia of Integer Sequences. Maybe you\\'ve\\xa0\\nencountered it yourself. You may recognize many\\xa0\\xa0 integer sequences just from memory like if I tell\\xa0\\nthe sequence 1, 1, 2, 3, 5, 8, 13 - you know that\\xa0\\xa0 is the Fibonacci sequence. The OEIS is a database\\xa0\\nof hundreds of thousands of sequences like this. Many times when a mathematician is working\\xa0\\non a research problem there is some natural\\xa0\\xa0 sequence of numbers associated. Maybe there\\'s\\xa0\\na sequence of spaces depending on n and you\\xa0\\xa0 compute the dimension or how many or the\\xa0\\ncardinality of a set or something. You can\\xa0\\xa0 compute the first five or six or 10 of these\\xa0\\nnumbers, put it in, and then compare it to\\xa0\\xa0 the OEIS. If you\\'re lucky this sequence has\\xa0\\nalready been put there by somebody else who\\xa0\\xa0 discovered it from a completely different\\xa0\\nsource, coming from studying some other\\xa0\\xa0 mathematical problem. That really gives you\\xa0\\na big clue that there\\'s a connection between\\xa0\\xa0 two problems and many promising productive\\xa0\\nresearch projects have come up that way. Tables are one of the earliest ways we\\'ve\\xa0\\nbeen using computers. The most famous when\\xa0\\xa0 you think of using computers to do mathematics\\xa0\\nyou think of number crunching. The formal name\\xa0\\xa0 for this is scientific computation. You want to\\xa0\\ndo a very big calculation and you just do lots\\xa0\\xa0 and lots of arithmetic - you source it out to a\\xa0\\ncomputer. We\\'ve been doing that since the 1920s. Maybe the first person to really do scientific\\xa0\\ncomputation was Hendrik Lorentz. He was tasked by\\xa0\\xa0 the Dutch to figure out what\\'s going to happen\\xa0\\n- they wanted to build a really giant dyke\\xa0\\xa0 and they wanted to know what happened to\\xa0\\nthe water flow and so they had to model\\xa0\\xa0 some fluid equations. He used a whole\\xa0\\nbunch of human computers actually to\\xa0\\xa0 work this out. He had to invent floating\\xa0\\npoint arithmetic to do this. He realized\\xa0\\xa0 that if you wanted to get a lot of people\\xa0\\nto do a lot of calculations very quickly\\xa0\\xa0 you should represent lots of numbers of\\xa0\\ndifferent magnitudes as floating points. We now use computers to model all kinds of things.\\xa0\\nIf you\\'re solving lots of linear equations or\\xa0\\xa0 partial differential equations or want to do\\xa0\\nsome combinatorial calculations, you can also\\xa0\\xa0 solve algebra problems. In principle, many of\\xa0\\nthe geometry questions you see at olympiads can\\xa0\\xa0 be solved in principle by scientific computation.\\xa0\\nThere are these algebra packages that can solve -\\xa0\\xa0 you can turn any geometry problem say involving 10\\xa0\\npoints and some lines and circles into a system of\\xa0\\xa0 equations of 20 real variables in some 20 unknowns\\xa0\\nand just whack it into Sage or Maple or something. Unfortunately, once it gets beyond a certain\\xa0\\nsize the complexity becomes exponential or\\xa0\\xa0 even double exponential. So until recently\\xa0\\nit was not really feasible to just brute\\xa0\\xa0 force these problems with just standard\\xa0\\ncomputer algebra packages but now with\\xa0\\xa0 AI assistance maybe it\\'s more promising.\\xa0\\nYou heard a talk about that this morning. Another type of scientific computation that\\xa0\\nhas become quite powerful is what are called\\xa0\\xa0 SAT solvers - satisfiability solvers. These\\xa0\\nare meant to solve kind of logic puzzles.\\xa0\\xa0 Like if you have 10 statements that are true\\xa0\\nor false - maybe a thousand statements that\\xa0\\xa0 are true or false - and you know that maybe\\xa0\\nif the third statement is true and the sixth\\xa0\\xa0 statement is true then the seventh statement\\xa0\\nmust be false. If you\\'re given a whole bunch\\xa0\\xa0 of constraints like that, a SAT solver\\xa0\\nwill try to take all this information\\xa0\\xa0 and conclude - can you prove a certain\\xa0\\ncombination of these sentences or not? There\\'s a more fancy version\\xa0\\nof a SAT solver called an SMT\\xa0\\xa0 solver - satisfiability modulo theories\\xa0\\n- where you also have some variables x,\\xa0\\xa0 y and z and you assume some laws like maybe\\xa0\\nthere\\'s an addition operation and addition\\xa0\\xa0 is commutative and associative. You plug in\\xa0\\nthese laws as well as some other facts and\\xa0\\xa0 you try to just brute force - can you deduce some\\xa0\\nconclusion out of some finite set of hypotheses? Those are quite powerful but unfortunately\\xa0\\nthey also don\\'t scale well at all. Again\\xa0\\xa0 the time complexity to solve them grows\\xa0\\nexponentially and so once you get past\\xa0\\xa0 a thousand or so propositions it becomes really\\xa0\\nhard for these solvers to run in any reasonable\\xa0\\xa0 amount of time. But they can actually solve\\xa0\\nsome problems. One recent success for example\\xa0\\xa0 here\\'s a problem that probably will only\\xa0\\never be solved by computer - this will\\xa0\\xa0 not be possible to solve by a human I think\\xa0\\nunassisted. It considers what was called the\\xa0\\xa0 Pythagorean triple problem which was unsolved\\xa0\\nuntil this big computer SAT solver calculation. The question is: You take the natural numbers and\\xa0\\nyou color them two colors red or blue. Is it true\\xa0\\xa0 that no matter how you color the natural numbers,\\xa0\\none of the colors must contain a Pythagorean\\xa0\\xa0 triple - three numbers which form the\\xa0\\nsides of a right triangle like 3, 4, 5? This was not known to be true. We have no\\xa0\\nsort of human proof of this. But we have\\xa0\\xa0 a computer proof. It is now known that\\xa0\\nin fact you don\\'t need all the natural\\xa0\\xa0 numbers - you just need to go up to\\xa0\\n7,824. No matter how many ways you\\xa0\\xa0 can color 7,824 into two color classes, one\\xa0\\nof them will contain a Pythagorean triple. Now there\\'s 2 to the 7824 such classes - you\\xa0\\ncan\\'t do it by brute force. So you have to\\xa0\\xa0 be somewhat clever. But it is possible. Once\\xa0\\nyou have 7,825, you must have a Pythagorean\\xa0\\xa0 triple. There was an example of 7,824 where\\xa0\\nthere\\'s no Pythagorean triple in either class. That\\'s provable. It actually I think it was\\xa0\\nthe world\\'s longest proof ever at the time. I\\xa0\\xa0 think now it\\'s the second longest\\xa0\\nproof. The proof required a few\\xa0\\xa0 years of computation and it generated\\xa0\\na proof certificate. The actual proof\\xa0\\xa0 is 200 terabytes long although it\\'s since\\xa0\\nbeen compressed to a mere 86 gigabytes. This is one way in which we can use\\xa0\\ncomputers just to do enormous case analysis. That\\'s sort of a fairly obvious way to use\\xa0\\ncomputers. But in recent years we\\'ve begun\\xa0\\xa0 to use computers in more creative ways. There are\\xa0\\nthree ways in which computers are being used to do\\xa0\\xa0 mathematics which I think are really exciting,\\xa0\\nparticularly when they are combined with each\\xa0\\xa0 other and with more classical databases, tables,\\xa0\\nand symbolic computation/scientific computation. First of all, we\\'re using machine learning\\xa0\\nneural networks to discover new connections\\xa0\\xa0 and find out ways in which different types\\xa0\\nof mathematics are correlated in ways that\\xa0\\xa0 you would not see as a human or\\xa0\\nare unlikely to see as a human. Most splashy are the large language models which\\xa0\\nare in some sense very large versions of machine\\xa0\\xa0 learning algorithms, which can take natural\\xa0\\nlanguage - like ChatGPT and Claude and so\\xa0\\xa0 forth - and they can sometimes generate possible\\xa0\\napproaches to problems which sometimes work,\\xa0\\xa0 sometimes don\\'t. You\\'ll see more examples\\xa0\\nof this actually in the talk after mine. There\\'s also another technology which is just\\xa0\\nbecoming usable by the everyday mathematician\\xa0\\xa0 which are called formal proof assistants.\\xa0\\nThese are languages - so you know languages,\\xa0\\xa0 computer languages you use to write executable\\xa0\\ncode, programs that do things - formal proof\\xa0\\xa0 assistants are languages that you write to check\\xa0\\nthings, to check whether a certain argument\\xa0\\xa0 is actually true and actually gives you the\\xa0\\nconclusion from the data. These have been fairly\\xa0\\xa0 annoying to use until very recently and they\\'re\\xa0\\nbecoming now somewhat easier to use and they are\\xa0\\xa0 facilitating a lot of interesting math projects\\xa0\\nthat wouldn\\'t have been possible without these\\xa0\\xa0 proof assistants. They will combine very well in\\xa0\\nthe future with the other tools I described here. So I want to talk about these more\\xa0\\nmodern ways to use machines and\\xa0\\xa0 computers to do mathematics. I think\\xa0\\nI\\'ll start with proof assistants. The first really computer-assisted proof maybe in\\xa0\\nhistory was the proof of the Four Color Theorem -\\xa0\\xa0 every planar map can be colored using only four\\xa0\\ncolors. That was proven in 1976. This was before\\xa0\\xa0 proof assistants - it wouldn\\'t really be called a\\xa0\\ncomputer proof nowadays. It was a proof which was\\xa0\\xa0 a massive computation, like half of which was done\\xa0\\nby computer and half of which was done by humans. The way they proved the Four Color Theorem\\xa0\\nis that you basically induct on the number\\xa0\\xa0 of countries. You show that if you have a massive\\xa0\\nmap, there\\'s some subgraph of countries - there\\xa0\\xa0 was a produced list of about 1,000-2,000\\xa0\\nspecial subgraphs - and every big graph of\\xa0\\xa0 countries had to contain in some sense one of\\xa0\\nthese subgraphs. That was one thing they had\\xa0\\xa0 to check. Then they had to check that every time\\xa0\\nyou had a subgraph you could replace that subgraph\\xa0\\xa0 with something simpler and if you could four-color\\xa0\\nthe simpler thing you could color the main thing. They had to check these properties - I\\xa0\\nthink they\\'re called dischargeability\\xa0\\xa0 and reducibility - for each of these 10,000\\xa0\\nor so subgraphs. I think one of these tasks\\xa0\\xa0 they could do by computer although this was an\\xa0\\nearly computer - I think they had to enter in\\xa0\\xa0 each graph by hand into this one program and\\xa0\\ncheck it. The other task was actually done by\\xa0\\xa0 a human computer - one of the daughters\\xa0\\nof one of the authors actually had to\\xa0\\xa0 spend hours and hours just manually checking\\xa0\\nthis reducibility thing. It was very tedious. The process was not perfect - there were lots\\xa0\\nof little mistakes and they had to update the\\xa0\\xa0 table. So it was not by modern standards\\xa0\\na computer proof or computer-verifiable\\xa0\\xa0 proof. That only came much later in the \\'90s\\xa0\\nwhere there was a simpler proof using a mere\\xa0\\xa0 700 or so graphs. But now all the things that\\xa0\\nneed to be checked - there was a very precise\\xa0\\xa0 well-defined list of properties and you could\\xa0\\nwrite code in your favorite computer language,\\xa0\\xa0 C or Python or something, and you\\xa0\\ncan check it in a couple pages and\\xa0\\xa0 a couple hundred lines of code in a\\xa0\\nfew minutes with a modern computer. To actually check that is completely -\\xa0\\nto write a proof that goes all the way\\xa0\\xa0 down to the axioms of mathematics\\xa0\\n- that was done in 2005 using a\\xa0\\xa0 proof assistant language called Coq.\\xa0\\nI think it\\'s now renamed to Rooster. That was one of the first proofs and you\\xa0\\nsee there\\'s a huge gap between sort of\\xa0\\xa0 when the proof first appeared and then when we\\xa0\\nactually could completely verify it by computer. Another famous example is the Kepler conjecture\\xa0\\nfor sphere packing. This is a really old\\xa0\\xa0 conjecture - Kepler from the 17th century. It\\'s\\xa0\\nvery easy to state: You take a whole bunch of unit\\xa0\\xa0 spheres and you want to cover three-dimensional\\xa0\\nspace as efficiently as possible. There\\'s sort of an obvious way to try to\\xa0\\npack spheres - it\\'s a triangular packing.\\xa0\\xa0 Like the way you pack oranges at a grocery\\xa0\\nstore. There\\'s also a dual packing called\\xa0\\xa0 the cubic packing which has the same\\xa0\\ndensity. There\\'s a density of about\\xa0\\xa0 74% which is the obvious packing and the\\xa0\\nquestion is - is that the best possible? This turns out to be a surprisingly\\xa0\\nhard problem. In two dimensions,\\xa0\\xa0 the hexagonal packing is not too hard to show\\xa0\\nit\\'s the best. Only very recently in 8 and 24\\xa0\\xa0 dimensions do we know the answer - great\\xa0\\nwork of Viazovska, maybe she talked about\\xa0\\xa0 it yesterday. But three was the only other case\\xa0\\nthat we know except for one which is trivial. Surprisingly difficult to prove. Again there\\xa0\\nwas no completely human-readable proof of\\xa0\\xa0 this conjecture. There is a strategy -\\xa0\\nso of course the problem is that there\\xa0\\xa0 are infinitely many of these spheres\\xa0\\nand the density is an asymptotic thing\\xa0\\xa0 so it\\'s not a priori a finite problem\\xa0\\nthat you can just throw at a computer. But you can try to reduce it to a finite\\xa0\\nproblem. There\\'s a strategy proposed by\\xa0\\xa0 Toth in the 50s. Every time you have a packing,\\xa0\\nit subdivides space into these polyhedra called\\xa0\\xa0 Voronoi regions. The Voronoi polytope of\\xa0\\na sphere is just all the points which are\\xa0\\xa0 closer to the center of that sphere than\\xa0\\nto all the other spheres. So you can sort\\xa0\\xa0 of split up space into all these polytopes and\\xa0\\nthese polytopes have certain volumes. You can\\xa0\\xa0 also count their faces and the surface areas and\\xa0\\nso forth and so they all have these statistics. The volumes - like the packing density\\xa0\\nis very closely related to sort of the\\xa0\\xa0 average volume of these regions. So if you\\xa0\\ncan say something about kind of how these\\xa0\\xa0 volumes of these polytopes behave on average,\\xa0\\xa0 then you could get at least maybe some upper\\xa0\\nbound onto how efficient these packings can be. You can try to make relations between\\xa0\\nthese polytopes - like if one polytope\\xa0\\xa0 is very big maybe it forces the nearby ones\\xa0\\nto be very small and so maybe you can try to\\xa0\\xa0 find some inequalities connecting the\\xa0\\nvolume of one polytope to another. So\\xa0\\xa0 maybe you should just collect lots and\\xa0\\nlots of these inequalities and then do\\xa0\\xa0 some linear programming or something\\xa0\\nand hopefully you can just derive the\\xa0\\xa0 right bound of this magic π/√18 which is the\\xa0\\nright density from all these inequalities. People tried this. There were many attempts - some\\xa0\\xa0 even claimed success. But none have\\xa0\\nbeen accepted as actual proofs. The problem was eventually solved first by Thomas\\xa0\\nHales and his co-authors. He did basically the\\xa0\\xa0 same strategy but with lots of technical tweaks.\\xa0\\nHe changed the cells from Voronoi cells to\\xa0\\xa0 slightly more fancy cells. Instead of taking\\xa0\\nthe volume he invented this called the score\\xa0\\xa0 that he assigned to each of these - it\\'s a volume\\xa0\\nplus or minus lots of little ad hoc adjustments. But again with the aim of trying to\\xa0\\ncreate all these linear inequalities\\xa0\\xa0 between these different scores and to eventually\\xa0\\xa0 get upper bounds of the density and to\\xa0\\nhopefully hit exactly the optimal density. It\\'s a very flexible method - actually it\\'s too\\xa0\\nflexible because there are too many things you\\xa0\\xa0 can try. There are so many ways you can set up\\xa0\\nthe score and so forth. So there\\'s a quote here:\\xa0\\xa0 \"Sam Ferguson realized that every time he\\xa0\\nencountered problems in trying to minimize\\xa0\\xa0 his functional and so forth he could\\xa0\\njust change the score and try it again.\" But then all the things that they checked\\xa0\\nalready they had to redo. And so the scoring\\xa0\\xa0 function became more and more complicated.\\xa0\\nYou know they worked on this for almost a\\xa0\\xa0 decade I think. It became more and more\\xa0\\ncomplicated but each change we cut months\\xa0\\xa0 through years from my work. This incessant\\xa0\\nfiddling was unpopular with our colleagues.\\xa0\\xa0 Every time I presented my work in progress\\xa0\\nat a conference I was minimizing a different\\xa0\\xa0 function. Even worse the function was\\xa0\\nmildly incompatible with what I did in\\xa0\\xa0 earlier papers and this required going\\xa0\\nback and patching the earlier papers. But eventually they did it. In 1998 they announced\\xa0\\xa0 that they had finally found a score\\xa0\\nwhich obeyed a whole bunch of linear\\xa0\\xa0 inequalities in 150 variables which\\xa0\\nthey did minimize and got their thing. Initially they did not plan to make this a\\xa0\\ncomputer-assisted proof but as the project became\\xa0\\xa0 more and more complicated it was inevitable\\xa0\\nthey had to use more and more computers. The proof was enormous by the standards of 1998.\\xa0\\nIt was 250 pages of notes and three gigabytes of\\xa0\\xa0 computer programs and data. It actually had a\\xa0\\nvery tough time getting refereed. It got sent\\xa0\\xa0 to the top journal Annals of Mathematics\\xa0\\nand it took four years to referee with a\\xa0\\xa0 panel of 12 referees. At the end they said\\xa0\\nthey were 99% certain of the correctness of\\xa0\\xa0 the proof but they could not certify the\\xa0\\ncorrectness of the computer calculations. They did a very unusual thing actually\\xa0\\n- they published the paper with a little\\xa0\\xa0 caveat from the editors saying this.\\xa0\\nThey have since removed that caveat\\xa0\\xa0 actually. At the time there was a lot more\\xa0\\ncontroversy as to whether a computer-assisted\\xa0\\xa0 proof qualified as an actual proof. Now I\\xa0\\nthink we are much more comfortable with it. But even after it was published there was doubt\\xa0\\nabout whether it was really a proof. So this\\xa0\\xa0 was maybe the first major high-profile problem\\xa0\\nwhere there was really a big incentive to really\\xa0\\xa0 formalize this completely all the way down to\\xa0\\nfirst principles in a formal proof language. So Hales in fact created a language\\xa0\\n- well a modification of existing\\xa0\\xa0 languages to do this. He called it the\\xa0\\nFlyspeck project. He estimated it would\\xa0\\xa0 take 20 years to formalize his proof but\\xa0\\nactually with the help of 21 collaborators\\xa0\\xa0 he actually finished in a mere 12\\xa0\\nyears. It finally appeared in 2014. So we now have sort of complete confidence in this\\xa0\\xa0 particular result but it was\\xa0\\nquite a painful thing to do. Moving now to the last few years - we\\'ve\\xa0\\nnow figured out sort of a better workflow\\xa0\\xa0 for how to formalize. It\\'s still\\xa0\\ntedious but it is getting better. Peter Scholze, who is a very prominent young\\xa0\\nmathematician - Fields Medalist for instance -\\xa0\\xa0 he\\'s famous for many many things but he created\\xa0\\nthis amazingly promising area of mathematics.\\xa0\\xa0 It\\'s called condensed mathematics. It deploys\\xa0\\nthe power of algebra, category theory and all\\xa0\\xa0 the tools from algebra to apply to functional\\xa0\\nanalysis - the theory of function spaces like\\xa0\\xa0 Banach spaces and so forth which in analysis has\\xa0\\nreally been resistant to the methods of algebra. But this area of mathematics in principle\\xa0\\ncould allow one to solve questions in at\\xa0\\xa0 least functional analysis - certain\\xa0\\ntypes of questions - with algebraic\\xa0\\xa0 methods. So he set up this whole category\\xa0\\nof these things called condensed abelian\\xa0\\xa0 groups and condensed vector spaces. I won\\'t\\xa0\\ntake long to explain what condensed means. His thesis is that all our categories of function\\xa0\\nspaces that we learn in our graduate classes are\\xa0\\xa0 incorrect or they\\'re not the natural ones\\xa0\\n- there are ones with better properties. So he set up this theory but there was this\\xa0\\none very important vanishing theorem which\\xa0\\xa0 he needed to prove. I stated it here but I\\'m\\xa0\\nnot going to explain what any of these words\\xa0\\xa0 mean or symbols mean. But there was a very\\xa0\\ntechnical vanishing of a certain category\\xa0\\xa0 theoretic group he needed to compute.\\xa0\\nWithout this the whole theory doesn\\'t\\xa0\\xa0 have any interesting consequences. This\\xa0\\nwas sort of the foundation of this theory. So he wrote a blog post about this result. He\\xa0\\nsaid he spent a whole year getting obsessed with\\xa0\\xa0 the proof of this theorem, going almost crazy over\\xa0\\nit. In the end we were able to get an argument put\\xa0\\xa0 down on paper but no one has dared to look at the\\xa0\\ndetails of this so I still have lingering doubts. \"With this theorem the hope that this condensed\\xa0\\nformalism can be fruitfully applied to functional\\xa0\\xa0 analysis stands or falls. This theorem is of the\\xa0\\nutmost foundational importance so being 99.9% sure\\xa0\\xa0 is not enough.\" He said he was happy to see many\\xa0\\nstudy groups on condensed mathematics throughout\\xa0\\xa0 the world but they all stopped short of the\\xa0\\nproof of this theorem. \"This proof is not much\\xa0\\xa0 fun.\" So he says \"This may be my most important\\xa0\\nresult to date - better be sure it\\'s correct.\" So he was also very incentivized\\xa0\\nto formalize this theorem now in a\\xa0\\xa0 more modern proof assistant language called Lean. Lean is a language that has been developed\\xa0\\nquite a lot in recent years. It comes with\\xa0\\xa0 a crowdsourced effort to develop this massive\\xa0\\nmath library. Rather than deriving everything\\xa0\\xa0 from the axioms of mathematics which becomes\\xa0\\nvery tedious the more advanced you go - and\\xa0\\xa0 this type of mathematics is very advanced - this\\xa0\\ncentral math library in Lean has already proved\\xa0\\xa0 lots of intermediate results like the type of\\xa0\\nthings you would see in say undergraduate math\\xa0\\xa0 courses like calculus or basic theorems\\xa0\\nof group theory or topology and so forth. These have already been formalized and\\xa0\\nso you have a standing base - you\\'re not\\xa0\\xa0 starting from the axioms, you\\'re\\xa0\\nstarting from roughly a graduate\\xa0\\xa0 level math education. Still a big gap\\xa0\\nto where you need to go but it helps. But in order to formalize this they had to add\\xa0\\nmany extra things. The math library was not\\xa0\\xa0 complete - it\\'s still not complete. There\\'s lots\\xa0\\nof areas of mathematics like homological algebra,\\xa0\\xa0 sheaf theory, topos theory that\\xa0\\nneeded to be added to the library. But in a mere 18 months they were\\xa0\\nable to formalize this theorem.\\xa0\\xa0 The proof was basically correct. There were\\xa0\\nsome minor technical issues but nothing really\\xa0\\xa0 major was discovered. They found some nice\\xa0\\nsimplifications. There were some technical\\xa0\\xa0 steps that were just too hard to formalize and\\xa0\\nso they were forced to find some shortcuts. But actually the value of this\\xa0\\nproject was more indirect. Firstly,\\xa0\\xa0 they greatly added to Lean\\'s math\\xa0\\nlibrary. So now this math library\\xa0\\xa0 can handle lots of abstract algebra to a\\xa0\\nmuch greater extent than it could before. But also there were other supporting\\xa0\\nsoftware that got set up that future\\xa0\\xa0 formalization projects have started\\xa0\\nusing, including some that I did. For example, one tool that was set\\xa0\\nup in the course of this project was\\xa0\\xa0 what\\'s called a blueprint. Taking a huge 50\\xa0\\npage proof and trying to directly formalize\\xa0\\xa0 it is really painful. You have to\\xa0\\nkeep the whole proof in your head. But what we\\'ve realized is the right workflow\\xa0\\nis that you take a big proof and you first write\\xa0\\xa0 what\\'s called a blueprint which sort of breaks\\xa0\\nup this proof into like hundreds of tiny little\\xa0\\xa0 steps. Each step you can formalize separately\\xa0\\nand then you just put them all together. So you try to break up a huge argument into lots\\xa0\\xa0 of little pieces. You write that first\\xa0\\nand then different people in your team\\xa0\\xa0 can formalize different parts of\\xa0\\ndifferent steps of your argument. So they also as a byproduct of this\\xa0\\nformalization also produced this very\\xa0\\xa0 nice blueprint. This is probably the\\xa0\\n- if you actually want to read the\\xa0\\xa0 proof as a human - the blueprint is\\xa0\\nprobably the best place to go to now. Another spin-off of this - so there\\'s now also\\xa0\\nthis formal proof which is tens of thousands\\xa0\\xa0 of lines long but now there are efforts to try\\xa0\\nto convert that back to a human readable proof. So another thing that\\'s been developed is\\xa0\\nthat there are now tools - you can take a\\xa0\\xa0 proof that\\'s been written in say this language\\xa0\\nLean - like here\\'s an example where there\\'s a\\xa0\\xa0 proof written of a topological problem and\\xa0\\nthey converted it back to a human readable\\xa0\\xa0 proof. So all this text here is a proof that\\xa0\\nis computer generated from a formal proof. It looks like a human proof. It uses the same sort\\xa0\\nof math language but it\\'s much more interactive.\\xa0\\xa0 You can click on any location - I can\\'t do it\\xa0\\nbecause it\\'s a static PDF - but you can click\\xa0\\xa0 on any location here and it will tell you wherever\\xa0\\nyou are what the hypothesis is, what you\\'re trying\\xa0\\xa0 to prove, what the variables are. If there\\'s\\xa0\\na step that is too short you can expand it and\\xa0\\xa0 it will explain where it came from and you can\\xa0\\ngo all the way down to the axioms if you want. I think this is great. I think in the\\xa0\\nfuture textbooks will be written in\\xa0\\xa0 this interactive style. You formalize them first\\xa0\\xa0 and then you can have much more\\xa0\\ninteractive textbooks than currently. Inspired by this, I myself started a\\xa0\\nproject to formalize - so I recently\\xa0\\xa0 last year I solved a problem in combinatorics\\xa0\\nwith several people including Tim Gowers who\\'s\\xa0\\xa0 here in the audience. It\\'s a problem in\\xa0\\ncombinatorics - it\\'s not super important\\xa0\\xa0 what the problem is. There\\'s a subset of Z mod\\xa0\\n2 to the n, like what\\'s called the Hamming cube,\\xa0\\xa0 and it obeys a property called small\\xa0\\ndoubling. Then there\\'s a certain limit\\xa0\\xa0 to how big it can be. But it doesn\\'t\\xa0\\nreally matter what the statement is. We proved it. The proof is about 33 pages. We\\xa0\\nformalized it in relatively record time - actually\\xa0\\xa0 probably still the fastest formalized actual\\xa0\\nresearch paper. In 3 weeks, in a group project of\\xa0\\xa0 about 20 people using all this blueprint machinery\\xa0\\nthat had been developed in Scholze\\'s project. It makes the task of proving things much\\xa0\\nmore open and collaborative. You get all\\xa0\\xa0 these nice visualizations. As I\\xa0\\nsaid, the first thing you do is\\xa0\\xa0 that you take your big theorem and you\\xa0\\nbreak it up into lots of little pieces. The theorem that we have is we call it PFR -\\xa0\\nwon\\'t explain why. That corresponds to this\\xa0\\xa0 little bubble at the bottom of this graph\\xa0\\nhere and then we introduce all these other\\xa0\\xa0 statements. The proof of PFR has to depend\\xa0\\non several other previous statements. These\\xa0\\xa0 ones depend on previous statements as\\xa0\\nwell. So there\\'s this dependency graph\\xa0\\xa0 and they have different colors depending\\xa0\\non whether you\\'ve formalized them or not. A green bubble is a statement that you\\'ve already\\xa0\\nformally proven in your language. A blue bubble\\xa0\\xa0 is one that hasn\\'t yet been formalized but it\\'s\\xa0\\nready to be formalized - like all the definitions\\xa0\\xa0 are in place, someone needs to actually go\\xa0\\nahead and do it. A white bubble - even the\\xa0\\xa0 statement hasn\\'t been formalized yet,\\xa0\\nsomeone has to write in the statement. So you get this tree of tasks. The\\xa0\\nbeauty of this project is that you\\xa0\\xa0 can get all these people to collaborate\\xa0\\non these different pieces of this graph\\xa0\\xa0 independently. Every little bubble\\xa0\\ncorresponds to some statement and\\xa0\\xa0 you don\\'t need to understand the whole proof\\xa0\\nin order to just work on your little piece. This was a problem in combinatorics\\xa0\\nbut the people who contributed - there\\xa0\\xa0 were people from probability, there were\\xa0\\npeople who were not even mathematicians,\\xa0\\xa0 they were computer programmers but they were\\xa0\\njust very good at sort of doing these little\\xa0\\xa0 mini puzzle type things. Everyone just sort of\\xa0\\npicked one bubble that they think they could\\xa0\\xa0 do and they did it. In three weeks we did the\\xa0\\nwhole thing. It was a really exciting project. In mathematics we don\\'t normally collaborate\\xa0\\nwith this many people. Maybe five people\\xa0\\xa0 is the most I\\'ve normally seen because when you\\xa0\\ncollaborate on a big project you have to trust\\xa0\\xa0 that everyone\\'s math is correct and past a certain\\xa0\\nsize this is just not feasible. But with a project\\xa0\\xa0 like this the Lean compiler automatically checks\\xa0\\n- you cannot upload anything that doesn\\'t compile,\\xa0\\xa0 it will get rejected. So you can collaborate\\xa0\\nwith people that you never met before. I met a lot of people actually - wrote\\xa0\\xa0 a lot of letters of recommendation\\xa0\\nactually coming out of this project. This is an example - like this is one\\xa0\\nlittle piece of the proof. This is what\\xa0\\xa0 a proof looks like in Lean. It\\'s\\xa0\\nnot exactly - I mean if you know\\xa0\\xa0 the language it\\'s human readable\\xa0\\nbut it looks a little bit unusual. It really sort of decouples the task\\xa0\\nof proving things into many different\\xa0\\xa0 sort of disjoint skills. You can have\\xa0\\nsome people who see the big picture and\\xa0\\xa0 organize things into little pieces\\xa0\\nand then you have people who don\\'t\\xa0\\xa0 necessarily know all the mathematics but\\xa0\\ncan just work on little pieces at a time. I think this will be a more and more common\\xa0\\nway of doing mathematics going forward. It\\xa0\\xa0 still is painful to do - like the tools\\xa0\\nare not really - they\\'re getting better\\xa0\\xa0 and user-friendlier but you still need\\xa0\\nto have some expertise in programming.\\xa0\\xa0 I would say it takes maybe 10 times longer to\\xa0\\nformalize a proof than to write it by hand. On the other hand if you want to\\xa0\\nchange a proof - so for example\\xa0\\xa0 there was a 12 that showed up in this theorem, we\\xa0\\nlater improved this 12 to an 11. We got a slightly\\xa0\\xa0 stronger theorem. Normally if you do that you have\\xa0\\nto rewrite the whole proof or like you could maybe\\xa0\\xa0 cut and paste 12 to 11 but then you have to check\\xa0\\nyou didn\\'t make any mistakes when you did that. But actually when we formalized this then\\xa0\\nwe got the improvement - it only took a\\xa0\\xa0 few days to change the theorem to the 11.\\xa0\\nWe just changed the 12 to 11 somewhere and\\xa0\\xa0 then the compiler complained like five\\xa0\\ndifferent places now is certain this\\xa0\\xa0 very specific part is not working and\\xa0\\nwe could just do some targeted fixes. So in fact for some specific\\xa0\\ntypes of doing mathematics\\xa0\\xa0 already the formal approach is actually faster. Now there are actually quite a few big proof\\xa0\\nformalization projects going on right now.\\xa0\\xa0 The biggest is Kevin Buzzard\\'s project - he\\'s just\\xa0\\ngot a big grant to formalize Fermat\\'s Last Theorem\\xa0\\xa0 in Lean. He says it will take five years to do\\xa0\\nthe most important parts of this proof. He doesn\\'t\\xa0\\xa0 claim to do the whole thing in five years but the\\xa0\\ninteresting part is already on the way actually. So that is formal proof assistants.\\xa0\\nI\\'ll talk about machine learning. Machine learning - these are using\\xa0\\nneural networks to predict answers\\xa0\\xa0 to various questions that you can use in\\xa0\\nmany ways. I think I\\'ll skip the first\\xa0\\xa0 way I was discussing which is to use\\xa0\\nneural networks to guess solutions to\\xa0\\xa0 differential equations which is a very\\xa0\\nexciting new tool in PDEs but I will skip it. I\\'ll talk about another application of\\xa0\\nmachine learning to knot theory. Knot\\xa0\\xa0 theory is quite a fun area of mathematics.\\xa0\\nIt\\'s an interesting area that brings together\\xa0\\xa0 many different fields of mathematics and\\xa0\\nthey don\\'t really talk to each other. A knot is just a loop of string or a curve\\xa0\\nreally in space that is closed. Two knots are\\xa0\\xa0 equivalent if there\\'s some way to continuously\\xa0\\ndeform one knot to another in a way in which\\xa0\\xa0 you\\'re not allowed to cross the string -\\xa0\\nthe string is not allowed to cross itself. The basic questions in knot theory\\xa0\\nare: When are two knots equivalent?\\xa0\\xa0 If I give you two knots is there\\xa0\\nsome way to turn one into the other? The way you approach this question normally\\xa0\\nis that you develop these things called knot\\xa0\\xa0 invariants. These are various numbers,\\xa0\\nsometimes also polynomials that you\\xa0\\xa0 can attach to a knot and these numbers don\\'t\\xa0\\nchange no matter how you continuously deform\\xa0\\xa0 the knot. So if two knots have different\\xa0\\ninvariants they cannot be equivalent. There are many many types of knot\\xa0\\ninvariants. There\\'s something\\xa0\\xa0 called the signature which counts - you\\xa0\\nflatten the knot and you count crossings,\\xa0\\xa0 whether the crossings go over or under and\\xa0\\nyou create a certain matrix and so forth\\xa0\\xa0 and you can get a certain integer called the\\xa0\\nsignature. That is one type of knot invariant. There are some famous polynomials called the\\xa0\\nJones polynomial and the Alexander polynomial\\xa0\\xa0 which are connected to many areas of\\xa0\\nmathematics but I won\\'t talk about that. Then there are these things called\\xa0\\nhyperbolic invariants which come from\\xa0\\xa0 geometry. You can take the complement\\xa0\\nof the knot and that is actually what\\'s\\xa0\\xa0 called a hyperbolic space. It comes\\xa0\\nwith a certain geometric structure,\\xa0\\xa0 has a notion of distance and you can compute\\xa0\\nits volume and some other invariants. These are invariants that are real\\xa0\\nor complex numbers. So every knot\\xa0\\xa0 comes with some combinatorial invariants\\xa0\\nlike signatures and it comes with these\\xa0\\xa0 geometric invariants like these hyperbolic\\xa0\\ninvariants. Here is a whole list of knots\\xa0\\xa0 with various hyperbolic invariants - there\\'s\\xa0\\nsomething called the hyperbolic volume and\\xa0\\xa0 the homological cusp shape and so forth.\\xa0\\nThese are real or complex numbers but no\\xa0\\xa0 one knew of any link between these two.\\xa0\\nThere were these two separate ways to\\xa0\\xa0 create statistics of knots and they didn\\'t\\xa0\\n- there was no connection between them. It was only very recently that people\\xa0\\nstarted using machine learning to attack\\xa0\\xa0 this problem. They created databases\\xa0\\nof millions of knots which actually\\xa0\\xa0 was already a slightly nontrivial task and\\xa0\\nthey trained a neural network on this. They\\xa0\\xa0 found that after training the neural network\\xa0\\nyou could give it all the hyperbolic geometry\\xa0\\xa0 invariants and like 90% of the time it will\\xa0\\npredict - it will guess the right signature. So it created this black box and it will\\xa0\\ntell you how the signature was somehow hidden\\xa0\\xa0 somewhere in these geometric invariants but it\\xa0\\ndidn\\'t tell you how - it was this black box. But that\\'s still useful because once you have this\\xa0\\nblack box you can just play with it. So what they\\xa0\\xa0 did next is actually very simple analysis - this\\xa0\\nis what\\'s called saliency analysis. What this\\xa0\\xa0 black box does - it takes about 20 different\\xa0\\ninputs, one for each hyperbolic invariant,\\xa0\\xa0 and which is one output - the signature. So\\xa0\\nonce you have this black box you can just tweak\\xa0\\xa0 each input. You just say what if I change one\\xa0\\ninput, how likely is it to change the output? Of the 20 inputs that they found, they\\xa0\\nfound that only three of them actually\\xa0\\xa0 played a really major role in the output.\\xa0\\nThe other 17 were barely relevant and it\\xa0\\xa0 wasn\\'t the three that they expected\\xa0\\nactually. They expected the volume\\xa0\\xa0 for example to be very important and the\\xa0\\nvolume turns out to be almost irrelevant. There were three - something called longitudinal\\xa0\\ntranslation and the real and complex parts of\\xa0\\xa0 meridional translation - there were these three\\xa0\\ninvariants that were the most important. So once\\xa0\\xa0 they identified the ones that were most important,\\xa0\\nthey could just plot directly the signature\\xa0\\xa0 against those three particular inputs and then\\xa0\\nthey could eyeball - rather than use neural\\xa0\\xa0 network they use the human network - to then\\xa0\\nsee oh okay there\\'s some obvious patterns here. By staring at these graphs they could\\xa0\\nactually make conjectures as to what was\\xa0\\xa0 actually going on. They made a conjecture\\xa0\\nbased on this which turned out to be wrong\\xa0\\xa0 actually. But they actually used the neural\\xa0\\nnetwork to show that it was wrong. But then\\xa0\\xa0 the way that it failed, they could correct\\xa0\\nit and they found a corrected version of\\xa0\\xa0 the conjecture which actually did explain\\xa0\\nthis phenomenon. Then once they found the\\xa0\\xa0 right statement they were able to prove\\xa0\\nit. So they actually have a theoretical\\xa0\\xa0 explanation of why the signature is so closely\\xa0\\nrelated to these particular statistics. This I think is a way in which machine\\xa0\\nlearning is being increasingly used in\\xa0\\xa0 mathematics. It doesn\\'t directly solve\\xa0\\nthe problem for you but it gives you all\\xa0\\xa0 these really useful hints as to where the\\xa0\\nconnections are and where to look at them,\\xa0\\xa0 but you still need the human to\\xa0\\nactually make the connections. And then finally we have the large language models\\xa0\\nwhich are the most splashy and have made the most\\xa0\\xa0 news. Neural networks have been around for 20\\xa0\\nyears but large language models have also been\\xa0\\xa0 around for 5 or so years but they\\'ve only become\\xa0\\nsort of human level in output very recently. You\\'ve all probably heard of GPT-4. This is\\xa0\\nChatGPT\\'s current model. Very famously when\\xa0\\xa0 GPT-4 came out there was a paper describing\\xa0\\nits capabilities and they fed it basically a\\xa0\\xa0 question from the 2022 IMO. It\\'s a slightly\\xa0\\nsimplified version - if you studied the 2022\\xa0\\xa0 IMO you\\'ll probably notice it\\'s not exactly\\xa0\\nthe same form but it\\'s a simplified form. For this particular question actually you give\\xa0\\nit the question and it actually gives a complete\\xa0\\xa0 correct solution to this question. It actually\\xa0\\nsolved an IMO question. Unfortunately this is\\xa0\\xa0 an extremely cherry-picked example. I think out of\\xa0\\nthe hundreds of IMO level questions they tested it\\xa0\\xa0 on, they had a success rate of about 1%. So this\\xa0\\nparticular problem they were able to solve, and\\xa0\\xa0 they had to format the problem in the right way to\\xa0\\nget the solution, but still this is quite amazing. On the other hand the funny thing about these\\xa0\\ntools is that things that humans find difficult,\\xa0\\xa0 AI can do very easily sometimes,\\xa0\\nbut things that humans find easy\\xa0\\xa0 AI often struggles with. It is a very\\xa0\\northogonal way of solving problems. In the same related paper or presentation, they\\xa0\\nasked the same model to do a basic arithmetic\\xa0\\xa0 computation: 7 * 4 + 8 * 8. The model,\\xa0\\nwhich is just guessing the most likely\\xa0\\xa0 output based on the input, basically guessed\\xa0\\nthe answer is 120. Then it paused and said\\xa0\\xa0 okay maybe I should give an explanation why\\xa0\\nit\\'s 120. So they did a step by step but\\xa0\\xa0 when they did a step by step they actually\\xa0\\narrived at the actual answer which is 92,\\xa0\\xa0 not the answer that they started with. So\\xa0\\nthen if you asked \"Wait but you said that\\xa0\\xa0 it was 120\" and they said \"Oh that was\\xa0\\na typo, sorry the correct answer is 92.\" So you know they\\'re not solving the problem\\xa0\\nfrom first principles, they\\'re just guessing at\\xa0\\xa0 each step of the output what is the most natural\\xa0\\nthing to say next. The amazing thing is sometimes\\xa0\\xa0 that works, but often it doesn\\'t. It\\'s still\\xa0\\nongoing how to sort of make it more accurate. People are trying all kinds of things.\\xa0\\nYou can connect these models to other\\xa0\\xa0 more reliable software. In fact you will see a\\xa0\\npresentation after where there\\'s a large language\\xa0\\xa0 model connected where you don\\'t do the computation\\xa0\\nyourself, you outsource it to Python in that case. But another thing you can do is that you\\xa0\\ncan force the language model to only produce\\xa0\\xa0 correct answers by forcing the model to output\\xa0\\nin one of these proof assistant languages and\\xa0\\xa0 if it doesn\\'t compile you send it back\\xa0\\nto the AI and the AI has to try again. Or you can try to teach it directly the\\xa0\\nsame problem solving techniques we use\\xa0\\xa0 to solve IMO problems - you know, try\\xa0\\nsimple examples, prove by contradiction,\\xa0\\xa0 try to actually prove step by step and so forth. So people are trying all kinds of things. It\\'s\\xa0\\nstill nowhere near able to solve a large majority\\xa0\\xa0 of say math olympiad problems, let alone math\\xa0\\nresearch problems, but we\\'re making progress. Besides being able to actually solve\\xa0\\nproblems directly, it\\'s also useful\\xa0\\xa0 just as a muse actually. I\\'ve also used these\\xa0\\nmodels myself. I\\'ve experimented with various\\xa0\\xa0 problems - I had a combinatorics problem\\xa0\\nwhich I was trying a few things and they\\xa0\\xa0 weren\\'t working so I as an experiment I just\\xa0\\ntried asking GPT \"What other techniques would\\xa0\\xa0 you suggest to solve this question?\" It\\xa0\\ngave me a list of 10 techniques of which\\xa0\\xa0 like 5 I\\'d already tried or were obviously\\xa0\\nnot helpful. But there was one technique\\xa0\\xa0 I\\'d not tried which was to use generating\\xa0\\nfunctions for this particular question,\\xa0\\xa0 which once it was suggested I realized it\\xa0\\nwas the right approach but I had missed it. So just as someone to converse with, it is\\xa0\\nsomewhat useful. It is not great right now\\xa0\\xa0 but it is not completely useless. There\\'s another type of AI assistance that\\'s\\xa0\\nactually become very useful for proof assistants.\\xa0\\xa0 As I said, writing formal proofs is a very tedious\\xa0\\ntask. I mean it\\'s like any really fussy computer\\xa0\\xa0 language - you have to get the syntax exactly\\xa0\\nright. If you miss a step it doesn\\'t compile. But there are tools - so I use something called\\xa0\\nGitHub Copilot where you can write down half of\\xa0\\xa0 a proof and it will try to guess what the next\\xa0\\nline is. About 20% of the time it actually\\xa0\\xa0 guesses something close to being correct and then\\xa0\\nyou can just say I\\'ll accept that and say okay. So in this case I was trying to prove this\\xa0\\nstatement here and the lines in gray are\\xa0\\xa0 the ones that Copilot suggested. It turns out\\xa0\\nthe first line is useless but the second line,\\xa0\\xa0 which you can\\'t quite see, actually\\xa0\\ndid solve this particular problem. So you still have to - you can\\'t just accept\\xa0\\nthe input because it won\\'t necessarily compile.\\xa0\\xa0 But if you already know sort of how the\\xa0\\ncode works it saves you a lot of time. These tools are getting better. So right\\xa0\\nnow they can maybe - if a proof is one\\xa0\\xa0 line or two lines long they can fill it in\\xa0\\nautomatically. There are now experiments to\\xa0\\xa0 sort of iterate an AI suggesting a proof\\xa0\\nand then you feed it back to the compiler\\xa0\\xa0 and then if it compiles wrong you send\\xa0\\nthe error message back. We\\'re beginning\\xa0\\xa0 to sort of prove things that are like 4 or 5\\xa0\\nlines long - they can be done by this method. Of course a big proof is like tens of thousands\\xa0\\nof lines so it\\'s nowhere near the point where you\\xa0\\xa0 can just instantly get your proof formalized\\xa0\\nimmediately. But it is already a useful tool. Okay so where are we now? There are people\\xa0\\nwho are hoping that in a few years we can\\xa0\\xa0 use computers to actually solve math problems\\xa0\\ndirectly. I think we are still a long way away\\xa0\\xa0 from that. For very narrowly focused problems\\xa0\\nyou can sort of set up specialized AI to handle\\xa0\\xa0 just a very narrow band of problems. But even then\\xa0\\nthey\\'re not fully reliable - they can be useful. But still for the next few years at\\xa0\\nleast, they\\'re basically going to be\\xa0\\xa0 really useful assistants beyond the sort\\xa0\\nof brute force computational assistance\\xa0\\xa0 that we\\'re already familiar with. People\\xa0\\nare trying all kinds of creative things. I think one direction which I find particularly\\xa0\\nexciting hasn\\'t really been successful yet -\\xa0\\xa0 hopefully AI will become very good at generating\\xa0\\ngood conjectures. We already saw a little example\\xa0\\xa0 of this with the knots where they could already\\xa0\\nsort of conjecture those connections between two\\xa0\\xa0 different statistics. So you know there\\'s just the\\xa0\\nhope that you just create these enormous datasets\\xa0\\xa0 and feed them into an AI and they would just\\xa0\\nautomatically generate lots of nice connections\\xa0\\xa0 between different mathematical objects. We don\\'t\\xa0\\nreally know how to do this yet partly because we\\xa0\\xa0 don\\'t have these massive datasets but I think this\\xa0\\nis something that would eventually be possible. One thing I\\'m also excited about - this is a\\xa0\\ntype of math that just doesn\\'t exist yet. Right\\xa0\\xa0 now because proving theorems is such a painful\\xa0\\npainstaking process we prove one theorem at a time\\xa0\\xa0 or maybe two or three if you\\'re efficient. But\\xa0\\nwith AI you could imagine in the future instead of\\xa0\\xa0 trying to prove one problem or solve one problem,\\xa0\\nyou take a class of 1,000 similar problems and\\xa0\\xa0 you say \"Okay I\\'m going to tell your AI try to\\xa0\\nsolve these 1,000 problems with this technique\"\\xa0\\xa0 and it will report back \"Oh I could solve 35% of\\xa0\\nthese problems with this technique. What about\\xa0\\xa0 this technique? I can solve this percentage\\xa0\\nof problems. Well if I combine them I can\\xa0\\xa0 do this.\" You could start exploring the space of\\xa0\\nproblems rather than just each problem separately. This is something that you just you\\xa0\\neither cannot do right now or you\\xa0\\xa0 do over a process of decades with dozens\\xa0\\nand dozens of papers slowly figuring out\\xa0\\xa0 what you can and can\\'t do with various\\xa0\\ntechniques. But with these tools you\\xa0\\xa0 could really start doing mathematics on\\xa0\\na scale which is really unprecedented. So the future is going to be really\\xa0\\nexciting Ithink. I mean they we will\\xa0\\xa0 still also be proving theorems the old\\xa0\\nfashioned way. In fact we\\'ll have to\\xa0\\xa0 because we can\\'t we won\\'t be able to guide\\xa0\\nthese AIs unless we also know how to do the\\xa0\\xa0 things ourselves. But we\\'ll be able to do\\xa0\\nlots of things that we can\\'t do right now. Okay I think I will stop there. So\\xa0\\nthank you very much. Any questions? So we are on a tight schedule but I\\xa0\\nwas told we have time for maybe three\\xa0\\xa0 or so questions. So if people would raise\\xa0\\nhands and - there\\'s someone over there. Thank you. Can you hear me? Thank you that was\\xa0\\na beautiful talk. I particularly loved about\\xa0\\xa0 formalizing mathematics but one thing that\\xa0\\nyou didn\\'t mention was Voevodsky who left\\xa0\\xa0 algebraic geometry because he made a mistake\\xa0\\nand started formalizing homotopy type theory.\\xa0\\xa0 I would be interested to know if you have\\xa0\\nstudied this and have any comments on it. Right. For Voevodsky - yeah he was\\xa0\\nworried about a crisis in certain\\xa0\\xa0 areas of mathematics including some\\xa0\\nthat he created, that the proofs were\\xa0\\xa0 so abstract and sophisticated that there was no\\xa0\\nway to verify that they were completely true. Yeah so he proposed changing the foundation\\xa0\\nof mathematics to homotopy type theory,\\xa0\\xa0 which is more robust. Like if you change\\xa0\\nthe underlying axioms of mathematics,\\xa0\\xa0 a lot of what you prove in\\xa0\\nthis theory is still true. There are proof assistant languages that\\xa0\\nare based on this sort of homotopy type\\xa0\\xa0 theory. Lean is not actually, by design,\\xa0\\nbecause Lean wants to formalize a lot\\xa0\\xa0 of traditional mathematics which\\xa0\\nis not written in this language. I do hope in the future there will be\\xa0\\nmultiple proof assistant languages with\\xa0\\xa0 different strengths and weaknesses. One thing\\xa0\\nthat we don\\'t have right now is automatic ways\\xa0\\xa0 to translate a proof in one language to another.\\xa0\\nThat\\'s actually one place where AI I think will\\xa0\\xa0 be very useful. Once we have that then we can\\xa0\\n- if you have a different philosophy of your\\xa0\\xa0 foundation of mathematics we could just hopefully\\xa0\\ntranslate a proof that\\'s been formalized in one\\xa0\\xa0 language to another and then everyone will\\xa0\\nbe convinced, including Voevodsky hopefully. Yeah so I mean there are multiple\\xa0\\napproaches to formalizing mathematics\\xa0\\xa0 and we shouldn\\'t just certainly fix\\xa0\\non one particular standard just yet. Well I - okay. All right so a random process.\\xa0\\nOkay, well that will not be quite relevant\\xa0\\xa0 to the topic of the talk, but I was recently\\xa0\\napplying for PhDs and the advice I was given\\xa0\\xa0 by the professors was basically along the lines of\\xa0\\n\"the longer the better\". So it seems like there\\'s\\xa0\\xa0 kind of a general agreement that mathematicians\\xa0\\nneed somehow to grow up to big ideas. So from\\xa0\\xa0 that perspective, how do you think about your\\xa0\\ndecision about going to university at such a\\xa0\\xa0 young age? Did you think - how it influenced\\xa0\\nyou as a mathematician and as a human being? Well I was - yeah I had some very good advisers,\\xa0\\xa0 both as a high school and undergraduate\\xa0\\nand graduate level. I mean I don\\'t think\\xa0\\xa0 it\\'s a race. I mean you go to university\\xa0\\nwhen you\\'re ready to go. You know you\\xa0\\xa0 shouldn\\'t go just because you were told\\xa0\\nyou need X years or something to do this. I think it\\'s different for different people. You\\xa0\\nknow I mean - it was very important for me. I mean\\xa0\\xa0 I went to undergraduate when I was 13 but it was\\xa0\\nat a university that was very close to where I\\xa0\\xa0 lived so I lived with my parents and they drove\\xa0\\nquite a lot actually to the university for all\\xa0\\xa0 my classes. If I didn\\'t have that I don\\'t\\xa0\\nthink I would have had a good experience. So it really depends. I mean okay I did my\\xa0\\nuniversity at a very young age - doesn\\'t\\xa0\\xa0 mean everybody should do that. Yeah it\\'s -\\xa0\\nthere\\'s no single answer to this question. Thank you. Okay so another kind of more general\\xa0\\nquestion: Given that you\\'ve contributed\\xa0\\xa0 to truly myriad mathematical fields,\\xa0\\nhow do you go about choosing your next\\xa0\\xa0 research topic and problem you want to\\xa0\\nsolve? And also what\\'s your Erdős number? Okay well my Erdős number is two, that\\'s easy.\\xa0\\nBut I actually don\\'t know - I mean it - I mean\\xa0\\xa0 early on in my career you know I had advisers who\\xa0\\nsuggested problems to me. Nowadays it often just\\xa0\\xa0 comes by serendipity. I mean math is a very\\xa0\\nsocial activity. You go to lots of events. So I mean after this I\\'m going to a math\\xa0\\nconference in Edinburgh and I\\'m going to talk\\xa0\\xa0 to a lot of people in actually an area connected\\xa0\\nto this PFR conjecture thing actually. Likely I\\'ll\\xa0\\xa0 have some interesting math conversations and\\xa0\\nmaybe some research questions come out of it. I do have some long-term projects that I\\xa0\\nwould like to - you know something that\\xa0\\xa0 I\\'d like to solve. But increasingly I find\\xa0\\nthat it\\'s the questions that just come up by\\xa0\\xa0 conversation with other mathematicians\\xa0\\nthat - yeah so you know I didn\\'t know\\xa0\\xa0 I\\'d be working - be talking so much about AI\\xa0\\nactually until about two years ago for instance. Yeah I think people should be sort of - you know\\xa0\\nthe future is going to require more flexibility.\\xa0\\xa0 I mean there will still be people who specialize\\xa0\\nin one topic and just one topic and be the world\\xa0\\xa0 expert in X, but increasingly I think there\\'ll\\xa0\\nbe more and more people who will move around over\\xa0\\xa0 time and just find interesting new mathematics\\xa0\\nevery few years just by talking to other people. Okay. I think we have to move\\xa0\\non actually. So the next speaker\\xa0\\xa0 is Simon Coyle from XTX who will\\xa0\\ntalk about the AI Math Olympiad.'\n"
     ]
    }
   ],
   "source": [
    "print(subtitle)\n",
    "print(repr(subtitle))  # This will display the raw text with all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = subtitle.replace(\"\\\\\", \"\")\n",
    "sentences = sent_tokenize(subtitle)\n",
    "#print(sentences)\n",
    "organized_sent = {k:v for v,k in enumerate(sentences)}\n",
    "tf_idf = TfidfVectorizer(min_df=2, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=True,\n",
    "                                    smooth_idf=True,\n",
    "                                    sublinear_tf=True,\n",
    "                                    stop_words = 'english')\n",
    "\n",
    "sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the scored sentences with their indexes as in the subtitle\n",
    "mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "# Ordering the top-n sentences in their original order\n",
    "mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "# joining the ordered sentence\n",
    "summary = \" \".join(ordered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They   found that after training the neural network \n",
      "you could give it all the hyperbolic geometry   invariants and like 90% of the time it will \n",
      "predict - it will guess the right signature. But if you already know sort of how the \n",
      "code works it saves you a lot of time. But we'll be able to do \n",
      "lots of things that we can't do right now.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55f970203394a91a322e2e249030931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c6d385d4474c6a9dca1d65da0ab4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the subtitle using Bart tokenizer\n",
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_tensor = model.generate(input_tensor, max_length=1024, min_length=120, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#outputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Professor Terence Tao is one of the most influential mathematicians of our time. He participated at the IMO for the first time when he was 11 years old. He was the youngest participant to receive a gold medal. His talk is on AI and more generally machine assistance in mathematics. It's all very exciting and it's beginning to be transformative, but on the other   hand there's also a sense of continuity, he says. He will be talking more about how these tools are beginning to change research mathematics which is different from competition mathematics. The talk will be followed by a presentation on the AI Math Olympia right after my talk.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs_tensor[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(link):\n",
    "    unique_id = link.split(\"=\")[-1]\n",
    "    sub = YouTubeTranscriptApi.get_transcript(unique_id)\n",
    "    subtitle = \" \".join([x['text'] for x in sub])\n",
    "    subtitle = subtitle.replace(\"\\\\\", \"\")\n",
    "    sentences = sent_tokenize(subtitle)\n",
    "    organized_sent = {k:v for v,k in enumerate(sentences)}\n",
    "    tf_idf = TfidfVectorizer(min_df=2, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=True,\n",
    "                                    smooth_idf=True,\n",
    "                                    sublinear_tf=True,\n",
    "                                    stop_words = 'english')\n",
    "\n",
    "    sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "    sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()\n",
    "    N = 3\n",
    "    top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]\n",
    "    mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "    mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "    ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "    summary = \" \".join(ordered_sentences)\n",
    "    return summary\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python summary",
   "language": "python",
   "name": "youtubesummarizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
