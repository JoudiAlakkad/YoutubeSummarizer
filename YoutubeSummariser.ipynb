{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the link\n",
    "link = \"https://www.youtube.com/watch?v=JSTUtRQ8Hwc\" \n",
    "# get the unique ID\n",
    "unique_id = link.split(\"=\")[-1]\n",
    "# get the transcript of the video based on the unique ID\n",
    "sub = YouTubeTranscriptApi.get_transcript(unique_id)  \n",
    "# get only the \"text\" parts and join them together, ignoring others like \"duration\"\n",
    "subtitle = \" \".join([x['text'] for x in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have five side projects that will give you an unfair Advantage at getting a job it's not another to-do list app or some Netflix clone it's an actual product for real paying customers so here's the deal if you build one of these products I will be your first paying customer I'm also looking for a founding engineer at my new company so if you build one of these projects I'll automatically consider you for an engineering role at my company if you'd like building a real product for real customers carries way more weight than you know whatever School you went to I'll give you some rules later on but let's just get into the ideas idea number one give GitHub productivity vs code extension let's talk about what everyone is most embarrassed about their GitHub activity monitor the thing basically shows how much you've coded in the last year people use it as a badge of honor to show how much they code go off there's even a side project that went viral on Twitter called ship talkers where they would show your ratio of tweets to commits I.E you know how much you talk versus how much you actually ship now there's a dirty little secret about the GitHub activity monitor that nobody likes to talk about it's actually a horrible way of determining how much someone has actually coded in the last year and here's why if you click into this button to see what they actually count as a contribution you'll see they only include things that were not on a fork so they had to be in the main repo but then more importantly they only count commits to the main branch so if it has to be merged into main to count then if you just raw dog all of your commits directly to main then your GitHub activity monitor is going to look great but if you meticulously think out and plan a PR over the course of a week and then you squash and merge it at the the end of it you get one contribution so obviously this is a subar way of tracking how much people code because it kind of promotes poor software engineering practices it's really just tailored for those people on Twitter that are Indie hackers that want to brag about their GitHub activity monitor but it would be nice if we actually had a way of showing how much people actually code here's what the GitHub activity monitor code tracking vs code extension is going to do one it's going to make a new git repo for you called code tracking two every 30 minutes it will make a commit into that branch that just has a quick summary of what you worked on for the last 30 minutes and so now you have a meticulous log of everything you did over every day for the last year as well as a contribution for every 30 minute segment that you actually Cod it heads down You' actually be surprised how handy this will also come in when you're at a job and they ask you to present why you should get a promotion and you all of a sudden forget everything you worked on for the last year having a repo that basically says everything you did every day it's actually a really good way of doing that and lastly this has to be an open- source project because people have to trust what you're doing with the code and with the information so if you build this and it's closed that sounds cool but it doesn't qualify if you build this I will be your first paying customer and I will pay you $10 a month for this product now obviously it's not life-changing money but I'm also going to try to advertise this for you to help you get some more paying customers okay project number two semantic search of my chat history when's the last time you you thought like oh my friend texted me the address I completely forget what did he what was it oh man my brother texted me that Netflix login username and password like a year ago like what was it again you have to scroll all the way back through your chat history kind of guessing where to look or you're just kind of like guessing keywords and searching them in your chat history but you're never finding the thing you're looking for it's horrible experience we all know it now imagine you had a single app where you could just make a search with normal language and it would search your slack messages your WhatApp messages your Facebook messages and it could just provide you the information so it's not just keyword search it's it's actual semantic search using artificial intelligence now if you decide to build a website like this I'll give you a few tips on it but it's super important that you're very transparent with people's data and that you have a very secure process so it's a difficult notion to actually scrape all this data from different sources from the get-go can't just be thrown together in a night because the stakes are pretty high so it has to be open source and you have to think through how you're going to collect people's data and let customers decide which data they want to share with you which chat histories they want to share with you and which ones they don't one way of of getting this information is to just use a Chrome extension to kind of scrape whenever someone's on that website you make a ton of requests to basically download the whole back catalog that sounds like a terrible idea in my opinion there's a gdpr mechanism in WhatsApp at least I know that you can request all of your data as part of gdpr companies have to let you to request to download all of your data and if you do that you just get all your chat history so I think that's probably the better way to go but but you you're going to have to explore this yourself because these are incredibly encrypted messages and to end encrypted WhatsApp is extremely private if you basically download their entire chat history and then save it in a database not encrypted that makes it all kind of useless so I really recommend that you think through your encryption strategy here and I if I was a beginner I wouldn't try this one basically what you're going to have to do is you're going to have to get embeddings for each chunk of user conversation and then you'll want to store the encrypted version next to embeddings and then use just normal Rag and encryption on top of some kind of llm so the way that looks is get the whole back catalog put it in a database one of the columns in the database is the embeddings for that if you need to learn more about embeddings through tons of resources online so then when someone makes a query you take that query you turn it into an embedding you can use that embedding to check your vector database to see all of the embeddings that are similar to it you pull all of that into your prompt and then you just ask the AI this is called rag I'm not going to go into it much more than that but there are plenty of resources online to learn more about rag either way the llm is also going to get to users data so you also have to be thoughtful about making sure that the data is deidentified if you're just plugging it into open AI or something so make it open source so we can take a look at it now if you get this to work because it's difficult I'm willing to pay you $30 a month for this product and I think it's a cool product and I know other people would use it too so okay actually this is a good time to stop and talk about the rules so obviously I can't pay everybody $10 a month that builds this project cuz if a 100 people build it I'm paying thousands of dollars a month obviously I can't do that that's why I'm going to have a website where I keep a rating of all the best entries for these project ideas I'll pay for the one that's currently the best this should also give you additional exposure to anyone watching this in the future if they think they want that solution too they can easily find it and buy the product from you now the way I want to build this website is using hostinger so that I don't have to code anything hosting or sponsoring this video so they're actually the ones paying you I'm going to use the money they pay me to pay you when you're building a product having a landing page is essential it's usually common practice to build a landing page in a drag or drop website like hostinger and the reason reason is like they make it really effortless for you and non-technical people to change these websites that way if you have someone else on your team in the future that wants to help you sell this product and they want to make some copy changes to the website they don't have to bother you while you work on the actual product it's a good separation of concerns bear in mind this is not the same as your actual application right you're going to use hostinger for the landing page to make it look pretty and easy to change but you're going to obviously have to code your own application you can't use a drag and drop product for that you want to spend a ton of time coding your website not coding your landing page and that's why hosting or a features come in really handy with the AI website builder you can create a unique website in minutes by just answering a few questions literally it takes like four clicks and your live they even have additional AI tools like their AI writer to help you craft SEO friendly content the AI logo maker to design a high quality logo the AI heat map to predict visitor behavior and optimize your site every plan even comes with a free domain and SSL CT I mean you're going to need a domain either way if you're making a website and if you want a cheap easy and beautiful website for your project just grab the Black Friday deal and use the codee Jason 10 to get 10% off and if you get the 48-month pricing for an additional 80% off this deal runs until December 15th you want a landing page for this new indie hacker project you're putting together if it's one of mine or even if it's not I highly recommend checking out hostinger they're doing us a big favor by helping us actually pay you guys for these websites so I think that's really cool and uh so I like hostinger they're they're good partners okay number three this one's a little bit more fun and less high stakes it's called win my argument it's Thanksgiving dinner you're right you're in a conversation with your sister she's saying something you disagree with her and you just want to prove your point so you say study show X and she says I don't believe that what studies and so you think actually what studies so you go to Google and you search your opinion and uh you just get a bunch of Reddit threads or you go to perplexity and you search your opinion and it basically does a really good job of also scraping those same Reddit threads and you go okay what's the use of this I'm actually looking for studies if you can build an artificial intelligence that scrapes millions of abstracts of actual studies and then reference those to help you prove your point when you're trying to make an argument and then put that into a application probably on your phone where you can just quickly ask it questions and get actual information from real studies now the best way to do this is also going to be rag you want to consume all the abstracts of every major study and then reference them so if you go to something like arxiv.org this website has millions and millions of studies and you can just use the AI to reference them directly you might say is that illegal or something I mean Google indexes the abstracts the abstracts are just there I don't think there's anything wrong with referencing the abstracts I think if you try to download the actual studies I think there's going to be some issues but all you're basically doing is building a better search engine specifically for studies so I don't think it's a big deal people will say Jason you can use perplexity for this but honestly I've tried this and it just really does not work very well it actually does pull up Reddit threads most of the time so this one should be a bit easier lower Stakes but also millions of studies to think about and how am I going to store all that data and blah blah blah so if you get this to work and it actually does work I will pay $15 a month for this product so the next one is called AWS supporter Chrome extension or something name it whatever you want now the problem is I use AWS all the time and AWS is a mess it actually is the worst UI of any product I've ever seen in my life I don't think I've ever said about a company that company needs more product managers I think that about AWS everybody knows it it's a mess it's documentation often is is out of date the butt to click nobody knows here's what I would love to do I would love to have a Chrome extension that has artificial intelligence baked in it creates a chat window next to my a instance and I can ask it questions not only will it be able to reference the docs I mean that's easy like even perplexity could do that but I wanted to actually highlight the buttons and the step-by-step process I need in order to execute something I want to do so for example I say uh how do I change the TLs configuration on you know this server and it should be able to just show me okay first click this button first search that then do this then do that and it should highlight the buttons and it should explain it as it goes through so that one's going to be difficult because you're going to have to actually think about where are all the buttons and how does the UI look so it's going to be tough but if you can execute on this one I'm willing to pay you $40 a month and I actually think that this has the potential to be one of the best products on this list because nobody knows how to use AWS even if they pretend that they do uh they know how to do like a handful of things but no one knows how to do everything on ws and imagine you actually did know how to do everything that would really unlock a lot of potential so number five this one is actually really fun if someone can do this I'd love this one this one's more of a wish Robin Hood made buying stocks really cool and easy you just tap on your phone you swipe up and you bought some stock and people got really into buying stocks because they've done it well what do developers love buying and buy way too much of and love showing them off but can't show them off domains I would love a quick and easy place to search domains on my phone and even if I use GoDaddy or name chep or one of their apps or something not only are their apps terrible but it doesn't give me a way to show them off or resell them really it doesn't even give me a way to quickly and easily resell them I think if you get the entirety of tech Twitter on a beautifully designed Sleek app that makes buying domains look and feel a lot like Robin Hood I think you could actually get a market of people buying and selling domains quickly just like people buy crypto now the core of this project is just make a beautiful interface for buying domains through a mobile app like it it isn't a huge bet it's not rocket science people will use it and if you do this I will buy all my domains through you and I will also try to sell domains for you the most I've ever spent on a domain is $3,000 for reference so you can bake in your own margins based on that number six this is the best one Aura ring for dogs so your dog I'm just kidding could you imagine no no obviously don't do that if you do that though that'd be pretty cool but I don't think uh I wouldn't use it be kind of silly so if you do any one of these projects I genuinely think it'll give you an unfair Advantage because it gives you an actual product that you have an actual paying customer and I will also by making this website try to help you get more paying customers now I hope to do more videos like this in the future so if there are other business ideas that you would genuinely pay for a solution to shoot them over to me add them in the Discord and I'll do them in the next video subscribe and I I hope that you build some of these projects\n"
     ]
    }
   ],
   "source": [
    "print(subtitle)\n",
    "#print(repr(subtitle))  # This will display the raw text with all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def summarise(text):\n",
    "\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a helpful AI assistent, Please provide a concise summary of the following text, highlighting the main points and key details while keeping the essence of the content intact. Here’s the text: \n",
    "    {text}\n",
    "    summarise the important \"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)  \n",
    "    prompt = prompt_template.format(text=subtitle)\n",
    "\n",
    "    model = Ollama(model=\"llama3.2\")  \n",
    "    summarised_text = model.invoke(prompt)\n",
    "    return summarised_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# encode the subtitle using Bart tokenizer\n",
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "outputs_tensor = model.generate(input_tensor, max_length=1024, min_length=120, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#outputs_tensor\n",
    "\n",
    "print(tokenizer.decode(outputs_tensor[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def summarize1(link):\n",
    "    unique_id = link.split(\"=\")[-1]\n",
    "    sub = YouTubeTranscriptApi.get_transcript(unique_id)\n",
    "    subtitle = \" \".join([x['text'] for x in sub])\n",
    "    subtitle = subtitle.replace(\"\\\\\", \"\")\n",
    "    sentences = sent_tokenize(subtitle)\n",
    "    organized_sent = {k:v for v,k in enumerate(sentences)}\n",
    "    tf_idf = TfidfVectorizer(min_df=2, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=True,\n",
    "                                    smooth_idf=True,\n",
    "                                    sublinear_tf=True,\n",
    "                                    stop_words = 'english')\n",
    "\n",
    "    sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "    sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()\n",
    "    N = 3\n",
    "    top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]\n",
    "    mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "    mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "    ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "    summary = \" \".join(ordered_sentences)\n",
    "    return summary\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python summary",
   "language": "python",
   "name": "youtubesummarizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
